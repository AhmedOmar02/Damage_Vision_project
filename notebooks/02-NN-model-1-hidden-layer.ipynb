{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c591f187-fb63-4bbc-bf1e-bc9ba46b4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "#import cupy as np\n",
    "#coudb't appy the code to the gpu using cupy I'll use other ways\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from logistic_regression.xml_utils import parse_cvat_xml_all_labels, label_Y_binary\n",
    "from logistic_regression.data_loader import load_and_resize_images, build_label_array\n",
    "from logistic_regression.model import model\n",
    "from logistic_regression.eval_utils import print_report\n",
    "from logistic_regression.visualization import plot_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6306eb05-e4a1-40a6-9db8-427de24fa074",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_XML = \"../EIDSeg_Dataset/data/train/train.xml\"\n",
    "TEST_XML = \"../EIDSeg_Dataset/data/test/test.xml\"\n",
    "TRAIN_IMAGES = \"../EIDSeg_Dataset/data/train/images/default\"\n",
    "TEST_IMAGES = \"../EIDSeg_Dataset/data/test/images/default\"\n",
    "\n",
    "IMAGE_SIZE = (64, 64)\n",
    "NUM_ITER = 2000\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3a2665-b502-4509-9b0d-f9837e37766a",
   "metadata": {},
   "source": [
    "## Load and parse XML labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76072720-20e9-40f1-b55d-e846dcb43dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_raw = parse_cvat_xml_all_labels(TRAIN_XML)\n",
    "labels_test_raw = parse_cvat_xml_all_labels(TEST_XML)\n",
    "\n",
    "Y_train_map = label_Y_binary(labels_train_raw)\n",
    "Y_test_map = label_Y_binary(labels_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed7e0d-72f6-4f27-9a57-98d26ac5ac00",
   "metadata": {},
   "source": [
    "## Load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2e219b-ee76-49a9-a51c-c5c47235eba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final X shape: (2612, 64, 64, 3)\n",
      "Final X shape: (327, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train_org, ordered_train = load_and_resize_images(TRAIN_IMAGES, size=IMAGE_SIZE)\n",
    "X_test_org, ordered_test = load_and_resize_images(TEST_IMAGES, size=IMAGE_SIZE)\n",
    "\n",
    "Y_train_org = build_label_array(ordered_train, Y_train_map)\n",
    "Y_test_org = build_label_array(ordered_test, Y_test_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158595a-f417-4132-8edd-51b89ffacf28",
   "metadata": {},
   "source": [
    "## Flatten Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45c22b5-65dc-4592-a95a-f56355a00144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 2612) (1, 2612)\n",
      "(12288, 327) (1, 327)\n"
     ]
    }
   ],
   "source": [
    "train_x = X_train_org.reshape(X_train_org.shape[0], -1).T\n",
    "test_x = X_test_org.reshape(X_test_org.shape[0], -1).T\n",
    "\n",
    "print(train_x.shape, Y_train_org.shape)\n",
    "print(test_x.shape, Y_test_org.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18812d5b-8e4f-4eb6-bfda-2776351afb68",
   "metadata": {},
   "source": [
    "## Defining the neural network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6f0d5a7-c4a0-4736-9389-599afba6e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f6b16a0-3f95-44b4-84c3-8815d613cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y, n_h=128):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    n_h -- number of nodes in the hidden layer\n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    #(≈ 3 lines of code)\n",
    "    # n_x = ... \n",
    "    # n_h = ...\n",
    "    # n_y = ... \n",
    "    # YOUR CODE STARTS HERE\n",
    "    n_x = np.shape(X)[0]\n",
    "    n_y= np.shape(Y)[0]\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8a249930-2b2d-4cad-8497-b2a108521d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y, seed=3):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"    \n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    np.random.seed(seed)\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13390dbb-5ed3-49d6-8c4a-325514cc1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    # (≈ 4 lines of code)\n",
    "    # Z1 = ...\n",
    "    # A1 = ...\n",
    "    # Z2 = ...\n",
    "    # A2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "     \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edf1ebbb-8cf9-452b-905f-4e543171e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of examples\n",
    "\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    # (≈ 2 lines of code)\n",
    "    # logprobs = ...\n",
    "    # cost = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    eps = 1e-8\n",
    "    first_half = np.dot(Y,np.log(A2 + eps).T)\n",
    "    socend_half= np.dot((1-Y),np.log(1-A2 + eps).T)\n",
    "    cost= first_half + socend_half\n",
    "    cost/=-m\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
    "                                    # E.g., turns [[17]] into 17 \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a347cdb-cb80-469b-b7b1-31cbf63c707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    #(≈ 2 lines of code)\n",
    "    # W1 = ...\n",
    "    # W2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    #(≈ 2 lines of code)\n",
    "    # A1 = ...\n",
    "    # A2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "        \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    #(≈ 6 lines of code, corresponding to 6 equations on slide above)\n",
    "    # dZ2 = ...\n",
    "    # dW2 = ...\n",
    "    # db2 = ...\n",
    "    # dZ1 = ...\n",
    "    # dW1 = ...\n",
    "    # db1 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dZ2 = A2 - Y \n",
    "    dW2 = np.dot(dZ2,A1.T)/m\n",
    "    db2 = np.sum(dZ2,axis=1,keepdims= True)/m\n",
    "    dZ1 = np.dot(W2.T,dZ2)*(1-np.power(A1,2))\n",
    "    dW1 = np.dot(dZ1,X.T)/m\n",
    "    db1 = np.sum(dZ1,axis=1,keepdims= True)/m\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5daa5d16-2089-4fd7-bd43-77e361d90828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve a copy of each parameter from the dictionary \"parameters\". Use copy.deepcopy(...) for W1 and W2\n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    W1 = copy.deepcopy(parameters[\"W1\"])\n",
    "    b1 = copy.deepcopy(parameters[\"b1\"])\n",
    "    W2 = copy.deepcopy(parameters[\"W2\"])\n",
    "    b2 = copy.deepcopy(parameters[\"b2\"])\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    #(≈ 4 lines of code)\n",
    "    # dW1 = ...\n",
    "    # db1 = ...\n",
    "    # dW2 = ...\n",
    "    # db2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    W1 += -learning_rate*dW1\n",
    "    b1 += -learning_rate*db1\n",
    "    W2 += -learning_rate*dW2\n",
    "    b2 += -learning_rate*db2\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cae8b55-4be8-44cf-a8b1-2231b81edfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Predict using the trained 2-layer network.\n",
    "    Returns Y_prediction (1, m) with 0/1 values.\n",
    "    \"\"\"\n",
    "    A2, _ = forward_propagation(X, parameters)\n",
    "    Y_prediction = (A2 > 0.5).astype(int)\n",
    "    return Y_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68d8cfcf-93ea-41a7-8be0-182679f39f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, Y):\n",
    "    return float(np.mean(predictions == Y) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07cca48-e33b-4763-9211-bd22a97bde82",
   "metadata": {},
   "source": [
    "## nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "888353db-9450-4bd7-ac28-a5e01ff88abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nn_model(X_train, Y_train, n_h=128, num_iterations = 10000, learning_rate=1.2, print_cost=False, X_test=None, Y_test=None):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X_train, Y_train)[0]\n",
    "    n_y = layer_sizes(X_train, Y_train)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    #(≈ 1 line of code)\n",
    "    # parameters = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    (n_x, n_h, n_y) = layer_sizes(X_train, Y_train)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        #(≈ 4 lines of code)\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        # A2, cache = ...\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y\". Outputs: \"cost\".\n",
    "        # cost = ...\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        # grads = ...\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        # parameters = ...\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        A2, cache = forward_propagation(X_train, parameters)\n",
    "\n",
    "        cost = compute_cost(A2, Y_train)\n",
    "        \n",
    "        grads = backward_propagation(parameters, cache, X_train, Y_train)\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and (i % 1000 == 0):\n",
    "            msg = f\"Cost after iteration {i}: {cost:.6f}\"\n",
    "            if X_test is not None and Y_test is not None:\n",
    "                train_pred = predict(parameters, X_train)\n",
    "                test_pred = predict(parameters, X_test)\n",
    "                train_acc = accuracy(train_pred, Y_train)\n",
    "                test_acc = accuracy(test_pred, Y_test)\n",
    "                msg += f\" | train acc: {train_acc:.2f}% | test acc: {test_acc:.2f}%\"\n",
    "            print(msg)\n",
    "    if X_test is not None and Y_test is not None:\n",
    "        train_pred = predict(parameters, X_train)\n",
    "        test_pred = predict(parameters, X_test)\n",
    "        train_acc = accuracy(train_pred, Y_train)\n",
    "        test_acc = accuracy(test_pred, Y_test)\n",
    "        print(f\"Final train accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"Final test accuracy:  {test_acc:.2f}%\")\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e32e8422-aa62-4f77-a710-e37833d35bb1",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "libcurand.so.10: cannot open shared object file: No such file or directory",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mImportError\u001b[39m                               Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[20]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m params = \u001b[43mnn_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train_org\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mn_h\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m128\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mprint_cost\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m                  \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mY_test_org\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[19]\u001b[39m\u001b[32m, line 23\u001b[39m, in \u001b[36mnn_model\u001b[39m\u001b[34m(X_train, Y_train, n_h, num_iterations, learning_rate, print_cost, X_test, Y_test)\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;66;03m# Initialize parameters\u001b[39;00m\n\u001b[32m     19\u001b[39m \u001b[38;5;66;03m#(≈ 1 line of code)\u001b[39;00m\n\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# parameters = ...\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# YOUR CODE STARTS HERE\u001b[39;00m\n\u001b[32m     22\u001b[39m (n_x, n_h, n_y) = layer_sizes(X_train, Y_train)\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m parameters = \u001b[43minitialize_parameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_y\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m \u001b[38;5;66;03m# YOUR CODE ENDS HERE\u001b[39;00m\n\u001b[32m     27\u001b[39m \n\u001b[32m     28\u001b[39m \u001b[38;5;66;03m# Loop (gradient descent)\u001b[39;00m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m0\u001b[39m, num_iterations):\n\u001b[32m     31\u001b[39m \n\u001b[32m     32\u001b[39m     \u001b[38;5;66;03m#(≈ 4 lines of code)\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     44\u001b[39m \n\u001b[32m     45\u001b[39m     \u001b[38;5;66;03m# YOUR CODE STARTS HERE\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 22\u001b[39m, in \u001b[36minitialize_parameters\u001b[39m\u001b[34m(n_x, n_h, n_y, seed)\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m      3\u001b[39m \u001b[33;03mArgument:\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[33;03mn_x -- size of the input layer\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     13\u001b[39m \u001b[33;03m                b2 -- bias vector of shape (n_y, 1)\u001b[39;00m\n\u001b[32m     14\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m    \n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m#(≈ 4 lines of code)\u001b[39;00m\n\u001b[32m     16\u001b[39m \u001b[38;5;66;03m# W1 = ...\u001b[39;00m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# b1 = ...\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     20\u001b[39m \u001b[38;5;66;03m# YOUR CODE STARTS HERE\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m#np.random.seed(seed)\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m W1 = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandom\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_h\u001b[49m\u001b[43m,\u001b[49m\u001b[43mn_x\u001b[49m\u001b[43m)\u001b[49m*\u001b[32m0.01\u001b[39m\n\u001b[32m     23\u001b[39m b1 = np.zeros((n_h,\u001b[32m1\u001b[39m))\n\u001b[32m     24\u001b[39m W2 = np.random.randn(n_y,n_h)*\u001b[32m0.01\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/My_projects /Damage_Vision_project/myenv1/lib/python3.12/site-packages/cupy/random/_sample.py:84\u001b[39m, in \u001b[36mrandn\u001b[39m\u001b[34m(*size, **kwarg)\u001b[39m\n\u001b[32m     81\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwarg:\n\u001b[32m     82\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[33mrandn() got unexpected keyword arguments \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m'\u001b[39m\n\u001b[32m     83\u001b[39m                     % \u001b[33m'\u001b[39m\u001b[33m, \u001b[39m\u001b[33m'\u001b[39m.join(kwarg.keys()))\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_distributions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnormal\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[43m=\u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/My_projects /Damage_Vision_project/myenv1/lib/python3.12/site-packages/cupy/random/_distributions.py:500\u001b[39m, in \u001b[36mnormal\u001b[39m\u001b[34m(loc, scale, size, dtype)\u001b[39m\n\u001b[32m    482\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mnormal\u001b[39m(loc=\u001b[32m0.0\u001b[39m, scale=\u001b[32m1.0\u001b[39m, size=\u001b[38;5;28;01mNone\u001b[39;00m, dtype=\u001b[38;5;28mfloat\u001b[39m):\n\u001b[32m    483\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Returns an array of normally distributed samples.\u001b[39;00m\n\u001b[32m    484\u001b[39m \n\u001b[32m    485\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    498\u001b[39m \n\u001b[32m    499\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m500\u001b[39m     rs = \u001b[43m_generator\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_random_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    501\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m rs.normal(loc, scale, size, dtype)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/My_projects /Damage_Vision_project/myenv1/lib/python3.12/site-packages/cupy/random/_generator.py:1307\u001b[39m, in \u001b[36mget_random_state\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m   1305\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m seed \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1306\u001b[39m         seed = numpy.uint64(\u001b[38;5;28mint\u001b[39m(seed))\n\u001b[32m-> \u001b[39m\u001b[32m1307\u001b[39m     rs = \u001b[43mRandomState\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1308\u001b[39m     rs = _random_states.setdefault(dev.id, rs)\n\u001b[32m   1309\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m rs\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/My_projects /Damage_Vision_project/myenv1/lib/python3.12/site-packages/cupy/random/_generator.py:57\u001b[39m, in \u001b[36mRandomState.__init__\u001b[39m\u001b[34m(self, seed, method)\u001b[39m\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, seed=\u001b[38;5;28;01mNone\u001b[39;00m, method=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mcupy_backends\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcuda\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlibs\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m curand\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m method \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m     60\u001b[39m         method = curand.CURAND_RNG_PSEUDO_DEFAULT\n",
      "\u001b[31mImportError\u001b[39m: libcurand.so.10: cannot open shared object file: No such file or directory"
     ]
    }
   ],
   "source": [
    "params = nn_model(train_x, Y_train_org,\n",
    "                  n_h=128,\n",
    "                  num_iterations=10000,\n",
    "                  learning_rate=1.2,\n",
    "                  print_cost=True,\n",
    "                  X_test=test_x,\n",
    "                  Y_test=Y_test_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8735ecf0-073b-4599-aaa7-1c94ef8510de",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944cea92-831b-43aa-bce4-8e3d00a78722",
   "metadata": {},
   "source": [
    "## The one-hidden-layer neural network outperformed logistic regression (79.33% train accuracy, 73.39% test accuracy), but accuracy remains below the required level. Training was also slow due to lack of GPU utilization. Future models will address these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c591f187-fb63-4bbc-bf1e-bc9ba46b4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "#import cupy as np\n",
    "#coudb't appy the code to the gpu using cupy I'll use other ways\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from logistic_regression.xml_utils import parse_cvat_xml_all_labels, label_Y_binary\n",
    "from logistic_regression.data_loader import load_and_resize_images, build_label_array\n",
    "from logistic_regression.model import model\n",
    "from logistic_regression.eval_utils import print_report\n",
    "from logistic_regression.visualization import plot_costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6306eb05-e4a1-40a6-9db8-427de24fa074",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_XML = \"../EIDSeg_Dataset/data/train/train.xml\"\n",
    "TEST_XML = \"../EIDSeg_Dataset/data/test/test.xml\"\n",
    "TRAIN_IMAGES = \"../EIDSeg_Dataset/data/train/images/default\"\n",
    "TEST_IMAGES = \"../EIDSeg_Dataset/data/test/images/default\"\n",
    "\n",
    "IMAGE_SIZE = (64, 64)\n",
    "NUM_ITER = 2000\n",
    "LR = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3a2665-b502-4509-9b0d-f9837e37766a",
   "metadata": {},
   "source": [
    "## Load and parse XML labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "76072720-20e9-40f1-b55d-e846dcb43dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train_raw = parse_cvat_xml_all_labels(TRAIN_XML)\n",
    "labels_test_raw = parse_cvat_xml_all_labels(TEST_XML)\n",
    "\n",
    "Y_train_map = label_Y_binary(labels_train_raw)\n",
    "Y_test_map = label_Y_binary(labels_test_raw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43ed7e0d-72f6-4f27-9a57-98d26ac5ac00",
   "metadata": {},
   "source": [
    "## Load and preprocess images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4a2e219b-ee76-49a9-a51c-c5c47235eba6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final X shape: (2612, 64, 64, 3)\n",
      "Final X shape: (327, 64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "X_train_org, ordered_train = load_and_resize_images(TRAIN_IMAGES, size=IMAGE_SIZE)\n",
    "X_test_org, ordered_test = load_and_resize_images(TEST_IMAGES, size=IMAGE_SIZE)\n",
    "\n",
    "Y_train_org = build_label_array(ordered_train, Y_train_map)\n",
    "Y_test_org = build_label_array(ordered_test, Y_test_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4158595a-f417-4132-8edd-51b89ffacf28",
   "metadata": {},
   "source": [
    "## Flatten Inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b45c22b5-65dc-4592-a95a-f56355a00144",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 2612) (1, 2612)\n",
      "(12288, 327) (1, 327)\n"
     ]
    }
   ],
   "source": [
    "train_x = X_train_org.reshape(X_train_org.shape[0], -1).T\n",
    "test_x = X_test_org.reshape(X_test_org.shape[0], -1).T\n",
    "\n",
    "print(train_x.shape, Y_train_org.shape)\n",
    "print(test_x.shape, Y_test_org.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18812d5b-8e4f-4eb6-bfda-2776351afb68",
   "metadata": {},
   "source": [
    "## Defining the neural network structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6f0d5a7-c4a0-4736-9389-599afba6e693",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9f6b16a0-3f95-44b4-84c3-8815d613cad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layer_sizes(X, Y, n_h=128):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- input dataset of shape (input size, number of examples)\n",
    "    Y -- labels of shape (output size, number of examples)\n",
    "    n_h -- number of nodes in the hidden layer\n",
    "    Returns:\n",
    "    n_x -- the size of the input layer\n",
    "    n_h -- the size of the hidden layer\n",
    "    n_y -- the size of the output layer\n",
    "    \"\"\"\n",
    "    #(≈ 3 lines of code)\n",
    "    # n_x = ... \n",
    "    # n_h = ...\n",
    "    # n_y = ... \n",
    "    # YOUR CODE STARTS HERE\n",
    "    n_x = np.shape(X)[0]\n",
    "    n_y= np.shape(Y)[0]\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    return (n_x, n_h, n_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8a249930-2b2d-4cad-8497-b2a108521d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_parameters(n_x, n_h, n_y, seed=3):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    n_x -- size of the input layer\n",
    "    n_h -- size of the hidden layer\n",
    "    n_y -- size of the output layer\n",
    "    \n",
    "    Returns:\n",
    "    params -- python dictionary containing your parameters:\n",
    "                    W1 -- weight matrix of shape (n_h, n_x)\n",
    "                    b1 -- bias vector of shape (n_h, 1)\n",
    "                    W2 -- weight matrix of shape (n_y, n_h)\n",
    "                    b2 -- bias vector of shape (n_y, 1)\n",
    "    \"\"\"    \n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    np.random.seed(seed)\n",
    "    W1 = np.random.randn(n_h,n_x)*0.01\n",
    "    b1 = np.zeros((n_h,1))\n",
    "    W2 = np.random.randn(n_y,n_h)*0.01\n",
    "    b2 = np.zeros((n_y,1))\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "\n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "13390dbb-5ed3-49d6-8c4a-325514cc1461",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def forward_propagation(X, parameters):\n",
    "    \"\"\"\n",
    "    Argument:\n",
    "    X -- input data of size (n_x, m)\n",
    "    parameters -- python dictionary containing your parameters (output of initialization function)\n",
    "    \n",
    "    Returns:\n",
    "    A2 -- The sigmoid output of the second activation\n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\"\n",
    "    \"\"\"\n",
    "    # Retrieve each parameter from the dictionary \"parameters\"\n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    b1 = parameters[\"b1\"]\n",
    "    b2 = parameters[\"b2\"]\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Implement Forward Propagation to calculate A2 (probabilities)\n",
    "    # (≈ 4 lines of code)\n",
    "    # Z1 = ...\n",
    "    # A1 = ...\n",
    "    # Z2 = ...\n",
    "    # A2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    \n",
    "    Z1 = np.dot(W1,X)+b1\n",
    "    A1 = np.tanh(Z1)\n",
    "    Z2 = np.dot(W2,A1) + b2\n",
    "    A2 = sigmoid(Z2)\n",
    "     \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    assert(A2.shape == (1, X.shape[1]))\n",
    "    \n",
    "    cache = {\"Z1\": Z1,\n",
    "             \"A1\": A1,\n",
    "             \"Z2\": Z2,\n",
    "             \"A2\": A2}\n",
    "    \n",
    "    return A2, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "edf1ebbb-8cf9-452b-905f-4e543171e781",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def compute_cost(A2, Y):\n",
    "    \"\"\"\n",
    "    Computes the cross-entropy cost given in equation (13)\n",
    "    \n",
    "    Arguments:\n",
    "    A2 -- The sigmoid output of the second activation, of shape (1, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "\n",
    "    Returns:\n",
    "    cost -- cross-entropy cost given equation (13)\n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "    m = Y.shape[1] # number of examples\n",
    "\n",
    "\n",
    "    # Compute the cross-entropy cost\n",
    "    # (≈ 2 lines of code)\n",
    "    # logprobs = ...\n",
    "    # cost = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    eps = 1e-8\n",
    "    first_half = np.dot(Y,np.log(A2 + eps).T)\n",
    "    socend_half= np.dot((1-Y),np.log(1-A2 + eps).T)\n",
    "    cost= first_half + socend_half\n",
    "    cost/=-m\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    cost = float(np.squeeze(cost))  # makes sure cost is the dimension we expect. \n",
    "                                    # E.g., turns [[17]] into 17 \n",
    "    \n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4a347cdb-cb80-469b-b7b1-31cbf63c707d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def backward_propagation(parameters, cache, X, Y):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation using the instructions above.\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing our parameters \n",
    "    cache -- a dictionary containing \"Z1\", \"A1\", \"Z2\" and \"A2\".\n",
    "    X -- input data of shape (2, number of examples)\n",
    "    Y -- \"true\" labels vector of shape (1, number of examples)\n",
    "    \n",
    "    Returns:\n",
    "    grads -- python dictionary containing your gradients with respect to different parameters\n",
    "    \"\"\"\n",
    "    m = X.shape[1]\n",
    "    \n",
    "    # First, retrieve W1 and W2 from the dictionary \"parameters\".\n",
    "    #(≈ 2 lines of code)\n",
    "    # W1 = ...\n",
    "    # W2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    W1 = parameters[\"W1\"]\n",
    "    W2 = parameters[\"W2\"]\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "        \n",
    "    # Retrieve also A1 and A2 from dictionary \"cache\".\n",
    "    #(≈ 2 lines of code)\n",
    "    # A1 = ...\n",
    "    # A2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    A1 = cache[\"A1\"]\n",
    "    A2 = cache[\"A2\"]\n",
    "        \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Backward propagation: calculate dW1, db1, dW2, db2. \n",
    "    #(≈ 6 lines of code, corresponding to 6 equations on slide above)\n",
    "    # dZ2 = ...\n",
    "    # dW2 = ...\n",
    "    # db2 = ...\n",
    "    # dZ1 = ...\n",
    "    # dW1 = ...\n",
    "    # db1 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dZ2 = A2 - Y \n",
    "    dW2 = np.dot(dZ2,A1.T)/m\n",
    "    db2 = np.sum(dZ2,axis=1,keepdims= True)/m\n",
    "    dZ1 = np.dot(W2.T,dZ2)*(1-np.power(A1,2))\n",
    "    dW1 = np.dot(dZ1,X.T)/m\n",
    "    db1 = np.sum(dZ1,axis=1,keepdims= True)/m\n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    grads = {\"dW1\": dW1,\n",
    "             \"db1\": db1,\n",
    "             \"dW2\": dW2,\n",
    "             \"db2\": db2}\n",
    "    \n",
    "    return grads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5daa5d16-2089-4fd7-bd43-77e361d90828",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def update_parameters(parameters, grads, learning_rate = 1.2):\n",
    "    \"\"\"\n",
    "    Updates parameters using the gradient descent update rule given above\n",
    "    \n",
    "    Arguments:\n",
    "    parameters -- python dictionary containing your parameters \n",
    "    grads -- python dictionary containing your gradients \n",
    "    \n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "    # Retrieve a copy of each parameter from the dictionary \"parameters\". Use copy.deepcopy(...) for W1 and W2\n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    W1 = copy.deepcopy(parameters[\"W1\"])\n",
    "    b1 = copy.deepcopy(parameters[\"b1\"])\n",
    "    W2 = copy.deepcopy(parameters[\"W2\"])\n",
    "    b2 = copy.deepcopy(parameters[\"b2\"])\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Retrieve each gradient from the dictionary \"grads\"\n",
    "    #(≈ 4 lines of code)\n",
    "    # dW1 = ...\n",
    "    # db1 = ...\n",
    "    # dW2 = ...\n",
    "    # db2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    dW1 = grads[\"dW1\"]\n",
    "    db1 = grads[\"db1\"]\n",
    "    dW2 = grads[\"dW2\"]\n",
    "    db2 = grads[\"db2\"]\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Update rule for each parameter\n",
    "    #(≈ 4 lines of code)\n",
    "    # W1 = ...\n",
    "    # b1 = ...\n",
    "    # W2 = ...\n",
    "    # b2 = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    W1 += -learning_rate*dW1\n",
    "    b1 += -learning_rate*db1\n",
    "    W2 += -learning_rate*dW2\n",
    "    b2 += -learning_rate*db2\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    parameters = {\"W1\": W1,\n",
    "                  \"b1\": b1,\n",
    "                  \"W2\": W2,\n",
    "                  \"b2\": b2}\n",
    "    \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6cae8b55-4be8-44cf-a8b1-2231b81edfde",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(parameters, X):\n",
    "    \"\"\"\n",
    "    Predict using the trained 2-layer network.\n",
    "    Returns Y_prediction (1, m) with 0/1 values.\n",
    "    \"\"\"\n",
    "    A2, _ = forward_propagation(X, parameters)\n",
    "    Y_prediction = (A2 > 0.5).astype(int)\n",
    "    return Y_prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "68d8cfcf-93ea-41a7-8be0-182679f39f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(predictions, Y):\n",
    "    return float(np.mean(predictions == Y) * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07cca48-e33b-4763-9211-bd22a97bde82",
   "metadata": {},
   "source": [
    "## nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "888353db-9450-4bd7-ac28-a5e01ff88abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nn_model(X_train, Y_train, n_h=128, num_iterations = 10000, learning_rate=1.2, print_cost=False, X_test=None, Y_test=None):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(3)\n",
    "    n_x = layer_sizes(X_train, Y_train)[0]\n",
    "    n_y = layer_sizes(X_train, Y_train)[2]\n",
    "    \n",
    "    # Initialize parameters\n",
    "    #(≈ 1 line of code)\n",
    "    # parameters = ...\n",
    "    # YOUR CODE STARTS HERE\n",
    "    (n_x, n_h, n_y) = layer_sizes(X_train, Y_train)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "\n",
    "    \n",
    "    # YOUR CODE ENDS HERE\n",
    "    \n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "         \n",
    "        #(≈ 4 lines of code)\n",
    "        # Forward propagation. Inputs: \"X, parameters\". Outputs: \"A2, cache\".\n",
    "        # A2, cache = ...\n",
    "        \n",
    "        # Cost function. Inputs: \"A2, Y\". Outputs: \"cost\".\n",
    "        # cost = ...\n",
    " \n",
    "        # Backpropagation. Inputs: \"parameters, cache, X, Y\". Outputs: \"grads\".\n",
    "        # grads = ...\n",
    " \n",
    "        # Gradient descent parameter update. Inputs: \"parameters, grads\". Outputs: \"parameters\".\n",
    "        # parameters = ...\n",
    "        \n",
    "        # YOUR CODE STARTS HERE\n",
    "        A2, cache = forward_propagation(X_train, parameters)\n",
    "\n",
    "        cost = compute_cost(A2, Y_train)\n",
    "        \n",
    "        grads = backward_propagation(parameters, cache, X_train, Y_train)\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        # YOUR CODE ENDS HERE\n",
    "        \n",
    "        # Print the cost every 1000 iterations\n",
    "        if print_cost and (i % 1000 == 0):\n",
    "            msg = f\"Cost after iteration {i}: {cost:.6f}\"\n",
    "            if X_test is not None and Y_test is not None:\n",
    "                train_pred = predict(parameters, X_train)\n",
    "                test_pred = predict(parameters, X_test)\n",
    "                train_acc = accuracy(train_pred, Y_train)\n",
    "                test_acc = accuracy(test_pred, Y_test)\n",
    "                msg += f\" | train acc: {train_acc:.2f}% | test acc: {test_acc:.2f}%\"\n",
    "            print(msg)\n",
    "    if X_test is not None and Y_test is not None:\n",
    "        train_pred = predict(parameters, X_train)\n",
    "        test_pred = predict(parameters, X_test)\n",
    "        train_acc = accuracy(train_pred, Y_train)\n",
    "        test_acc = accuracy(test_pred, Y_test)\n",
    "        print(f\"Final train accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"Final test accuracy:  {test_acc:.2f}%\")\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e32e8422-aa62-4f77-a710-e37833d35bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.690161 | train acc: 70.64% | test acc: 74.01%\n",
      "Cost after iteration 1000: 0.536566 | train acc: 74.12% | test acc: 75.84%\n",
      "Cost after iteration 2000: 0.488770 | train acc: 80.93% | test acc: 68.50%\n",
      "Cost after iteration 3000: 0.425843 | train acc: 85.64% | test acc: 63.30%\n",
      "Final train accuracy: 79.33%\n",
      "Final test accuracy:  73.39%\n"
     ]
    }
   ],
   "source": [
    "params = nn_model(train_x, Y_train_org,\n",
    "                  n_h=128,\n",
    "                  num_iterations=4000,\n",
    "                  learning_rate=0.01,\n",
    "                  print_cost=True,\n",
    "                  X_test=test_x,\n",
    "                  Y_test=Y_test_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef19e6ec-8692-4a92-bea5-dca888d8912a",
   "metadata": {},
   "source": [
    "The one-hidden-layer neural network outperformed logistic regression (79.33% train accuracy, 73.39% test accuracy), but accuracy remains below the required level. Training was also slow due to lack of GPU utilization. Future models will address these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc2336-866a-43c8-96a3-35e7558a0baa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

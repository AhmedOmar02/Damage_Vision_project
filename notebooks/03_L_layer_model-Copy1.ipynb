{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d01ecebc-c031-4585-84bc-c386bf069372",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py \n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from L_layer_model_03.L_layer_model_old import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b73a148f-7cba-472f-ab71-1e177d2aa7d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 2612\n",
      "Number of testing examples: m_test = 327\n",
      "Height/Width of each image: num_px = 32\n",
      "Each image is of size: (32, 32, 3)\n",
      "train_set_x shape: (2612, 32, 32, 3)\n",
      "train_set_y shape: (1, 2612)\n",
      "test_set_x shape: (327, 32, 32, 3)\n",
      "test_set_y shape: (1, 327)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"../EIDSeg_Dataset/cache/eidseg_32x32_binary_any.h5\", \"r\") as f:\n",
    "    X_train_org = f[\"X_train\"][:]\n",
    "    Y_train_org = f[\"Y_train\"][:]\n",
    "    X_test_org  = f[\"X_test\"][:]\n",
    "    Y_test_org  = f[\"Y_test\"][:]\n",
    "\n",
    "m_train = X_train_org.shape[0]\n",
    "m_test = X_test_org.shape[0]\n",
    "num_px =X_train_org.shape[1]\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(X_train_org.shape))\n",
    "print (\"train_set_y shape: \" + str(Y_train_org.shape))\n",
    "print (\"test_set_x shape: \" + str(X_test_org.shape))\n",
    "print (\"test_set_y shape: \" + str(Y_test_org.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "22e637d2-90ca-4ecd-825d-7ae337d1e5e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x flatten shape: (3072, 2612)\n",
      "train_set_y shape: (1, 2612)\n",
      "test_set_x flatten shape: (3072, 327)\n",
      "test_set_y shape: (1, 327)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"../EIDSeg_Dataset/cache//eidseg_32x32_binary_any_flat.h5\", \"r\") as f:\n",
    "    train_x = f[\"train_x\"][:]   # (12288, m)\n",
    "    train_y = f[\"train_y\"][:]   # (1, m)\n",
    "    test_x  = f[\"test_x\"][:]    # (12288, m)\n",
    "    test_y  = f[\"test_y\"][:]    # (1, m)\n",
    "\n",
    "print (\"train_set_x flatten shape: \" + str(train_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_y.shape))\n",
    "print (\"test_set_x flatten shape: \" + str(test_x.shape))\n",
    "print (\"test_set_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1df42422-8432-4c33-9d0d-f729c0650a93",
   "metadata": {},
   "source": [
    "## L-layer Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f92311b7-7e2c-4b5a-b330-70c73fb366fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False, X_test=None, Y_test=None):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    # np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    pos_weight = (Y.shape[1] - np.sum(Y)) / np.sum(Y)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "                \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL, Y, pos_weight)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        # Update parameters.\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                        \n",
    "        # Print the cost every 100 iterations and for the last iteration\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    if X_test is not None and Y_test is not None:\n",
    "        train_pred = predict(X, parameters)\n",
    "        test_pred  = predict(X_test, parameters)\n",
    "    \n",
    "        train_acc = accuracy(train_pred, Y)\n",
    "        test_acc  = accuracy(test_pred, Y_test)\n",
    "    \n",
    "        print(f\"Final train accuracy: {train_acc*100:.2f}%\")\n",
    "        print(f\"Final test accuracy:  {test_acc*100:.2f}%\")\n",
    "\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f0d888c1-c55f-489e-b7ff-bfa833da5c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå Backprop error detected! difference = 2.7552063513895956e-07\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [12288, 64, 16, 5, 1] \n",
    "\n",
    "X_gc = train_x[:, :5]\n",
    "Y_gc = train_y[:, :5]\n",
    "\n",
    "parameters = initialize_parameters_deep(layers_dims)\n",
    "AL, caches = L_model_forward(X_gc, parameters)\n",
    "grads = L_model_backward(AL, Y_gc, caches)\n",
    "\n",
    "difference = gradient_check_L_layer(\n",
    "    parameters,\n",
    "    grads,\n",
    "    X_gc,\n",
    "    Y_gc,\n",
    "    layers_dims\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b9617684-0e0f-44d3-8a2b-af01027a3c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.40606203936117047\n",
      "Cost after iteration 100: 0.42868061249762585\n",
      "Cost after iteration 200: 0.4487868714200463\n",
      "Cost after iteration 300: 0.45693217079428844\n",
      "Cost after iteration 400: 0.45970947014073893\n",
      "Cost after iteration 500: 0.4605803593548569\n",
      "Cost after iteration 600: 0.4608911736561531\n",
      "Cost after iteration 700: 0.4610149602226259\n",
      "Cost after iteration 800: 0.4610567191577916\n",
      "Cost after iteration 900: 0.4610665561205566\n",
      "Cost after iteration 1000: 0.4610697189655372\n",
      "Cost after iteration 1100: 0.4610738425992954\n",
      "Cost after iteration 1200: 0.46107243414960986\n",
      "Cost after iteration 1300: 0.461072363355223\n",
      "Cost after iteration 1400: 0.46106943788582705\n",
      "Cost after iteration 1500: 0.4610670750598367\n",
      "Cost after iteration 1600: 0.4610647190840069\n",
      "Cost after iteration 1700: 0.46106157820437943\n",
      "Cost after iteration 1800: 0.4610580083148715\n",
      "Cost after iteration 1900: 0.4610549502640192\n",
      "Cost after iteration 2000: 0.46105227320018577\n",
      "Cost after iteration 2100: 0.46104950952438223\n",
      "Cost after iteration 2200: 0.46104520742927874\n",
      "Cost after iteration 2300: 0.4610426979551611\n",
      "Cost after iteration 2400: 0.4610407703875496\n",
      "Cost after iteration 2499: 0.46103848695901006\n",
      "Final train accuracy: 70.71%\n",
      "Final test accuracy:  74.01%\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [3072, 256, 64, 16, 1] \n",
    "# parameters, costs =  model(train_x, train_y, learning_rate = 0.008, num_iterations = 2000, print_cost = True)\n",
    "parameters, costs = L_layer_model(train_x, train_y, layers_dims, learning_rate = 0.008,num_iterations = 2500, print_cost = True, X_test=test_x, Y_test=test_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fee6ba60-9a4c-41c8-9e92-e66a74306cc1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique preds: (array([1.]), array([2612]))\n"
     ]
    }
   ],
   "source": [
    "preds = predict(train_x, parameters)\n",
    "print(\"Unique preds:\", np.unique(preds, return_counts=True))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "178d89ef-dd73-4c77-866c-79384ded9dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Model Evaluation Report ===\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAHHCAYAAACiOWx7AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy81sbWrAAAACXBIWXMAAA9hAAAPYQGoP6dpAABSDUlEQVR4nO3deVxU9f4/8NfMMDOArMqOKIi54ormRa9LibiWVvdGZWV2r9cUyqXFrJtmt7Su5bWbluX3Z3Yry7TUcg1RrNzBFRcQBVHZRIRhZ5j5/P7AGR0BHXCYMwOv5+PBA+dzPufM+3wkfHXOZz5HJoQQICIiIqI7kktdABEREZE9YGgiIiIiMgNDExEREZEZGJqIiIiIzMDQRERERGQGhiYiIiIiMzA0EREREZmBoYmIiIjIDAxNRERERGZgaCKiOj333HMIDg5u1L5vv/02ZDKZZQsiIpIYQxORnZHJZGZ9JSQkSF2qJJ577jm4uLhIXYbZNmzYgNGjR8PLywsqlQoBAQF4/PHHsWvXLqlLI6LbyPjsOSL78s0335i8/t///oe4uDh8/fXXJu0jRoyAr69vo99Hq9VCr9dDrVY3eN/q6mpUV1fD0dGx0e/fWM899xzWr1+PkpISq793Qwgh8Pzzz2P16tXo06cP/vKXv8DPzw/Z2dnYsGEDkpKSsHfvXgwcOFDqUonoBgepCyCihnn66adNXh84cABxcXG12m9XVlYGZ2dns99HqVQ2qj4AcHBwgIMDf73cyUcffYTVq1dj5syZWLJkicntzDfffBNff/21RcZQCIGKigo4OTnd87GIWjreniNqhoYNG4awsDAkJSVhyJAhcHZ2xhtvvAEA2LRpE8aOHYuAgACo1WqEhobiX//6F3Q6nckxbp/TlJGRAZlMhg8//BBffPEFQkNDoVar0b9/fxw+fNhk37rmNMlkMsTGxmLjxo0ICwuDWq1G9+7dsX379lr1JyQkoF+/fnB0dERoaCg+//xzi8+TWrduHcLDw+Hk5AQvLy88/fTTuHLlikmfnJwcTJ48GW3btoVarYa/vz/Gjx+PjIwMY5/ExESMHDkSXl5ecHJyQkhICJ5//vk7vnd5eTkWLVqELl264MMPP6zzvJ555hncf//9AOqfI7Z69WrIZDKTeoKDgzFu3Djs2LED/fr1g5OTEz7//HOEhYXhgQceqHUMvV6PwMBA/OUvfzFpW7p0Kbp37w5HR0f4+vpi6tSpuH79+h3Pi6i54/8KEjVT165dw+jRo/HEE0/g6aefNt6qW716NVxcXDB79my4uLhg165dmDdvHjQaDRYvXnzX465ZswbFxcWYOnUqZDIZ/v3vf+PRRx/FhQsX7np16o8//sBPP/2E6dOnw9XVFf/973/x2GOPITMzE23atAEAHD16FKNGjYK/vz8WLFgAnU6Hd955B97e3vc+KDesXr0akydPRv/+/bFo0SLk5ubi448/xt69e3H06FF4eHgAAB577DGcOnUKL774IoKDg5GXl4e4uDhkZmYaX0dFRcHb2xuvv/46PDw8kJGRgZ9++umu41BQUICZM2dCoVBY7LwMUlJS8OSTT2Lq1KmYMmUKOnfujOjoaLz99tvIycmBn5+fSS1ZWVl44oknjG1Tp041jtFLL72E9PR0LFu2DEePHsXevXvv6SokkV0TRGTXYmJixO3/KQ8dOlQAECtWrKjVv6ysrFbb1KlThbOzs6ioqDC2TZo0SbRv3974Oj09XQAQbdq0EQUFBcb2TZs2CQDil19+MbbNnz+/Vk0AhEqlEmlpaca248ePCwDik08+MbY99NBDwtnZWVy5csXYdu7cOeHg4FDrmHWZNGmSaNWqVb3bq6qqhI+PjwgLCxPl5eXG9s2bNwsAYt68eUIIIa5fvy4AiMWLF9d7rA0bNggA4vDhw3et61Yff/yxACA2bNhgVv+6xlMIIb788ksBQKSnpxvb2rdvLwCI7du3m/RNSUmpNdZCCDF9+nTh4uJi/Ln4/fffBQDx7bffmvTbvn17ne1ELQlvzxE1U2q1GpMnT67VfuvcluLiYuTn52Pw4MEoKyvD2bNn73rc6OhoeHp6Gl8PHjwYAHDhwoW77hsZGYnQ0FDj6549e8LNzc24r06nw86dOzFhwgQEBAQY+3Xs2BGjR4++6/HNkZiYiLy8PEyfPt1kovrYsWPRpUsXbNmyBUDNOKlUKiQkJNR7W8pwRWrz5s3QarVm16DRaAAArq6ujTyLOwsJCcHIkSNN2jp16oTevXtj7dq1xjadTof169fjoYceMv5crFu3Du7u7hgxYgTy8/ONX+Hh4XBxccHu3bubpGYie8DQRNRMBQYGQqVS1Wo/deoUHnnkEbi7u8PNzQ3e3t7GSeRFRUV3PW67du1MXhsClDnzXW7f17C/Yd+8vDyUl5ejY8eOtfrV1dYYFy9eBAB07ty51rYuXboYt6vVanzwwQfYtm0bfH19MWTIEPz73/9GTk6Osf/QoUPx2GOPYcGCBfDy8sL48ePx5ZdforKy8o41uLm5AagJrU0hJCSkzvbo6Gjs3bvXOHcrISEBeXl5iI6ONvY5d+4cioqK4OPjA29vb5OvkpIS5OXlNUnNRPaAoYmomarr01KFhYUYOnQojh8/jnfeeQe//PIL4uLi8MEHHwComQB8N/XNwRFmrF5yL/tKYebMmUhNTcWiRYvg6OiIt956C127dsXRo0cB1ExuX79+Pfbv34/Y2FhcuXIFzz//PMLDw++45EGXLl0AACdPnjSrjvomwN8+ed+gvk/KRUdHQwiBdevWAQB++OEHuLu7Y9SoUcY+er0ePj4+iIuLq/PrnXfeMatmouaIoYmoBUlISMC1a9ewevVqzJgxA+PGjUNkZKTJ7TYp+fj4wNHREWlpabW21dXWGO3btwdQM1n6dikpKcbtBqGhoXj55Zfx66+/Ijk5GVVVVfjoo49M+vzpT3/Ce++9h8TERHz77bc4deoUvv/++3pr+POf/wxPT09899139QafWxn+fgoLC03aDVfFzBUSEoL7778fa9euRXV1NX766SdMmDDBZC2u0NBQXLt2DYMGDUJkZGStr169ejXoPYmaE4YmohbEcKXn1is7VVVV+PTTT6UqyYRCoUBkZCQ2btyIrKwsY3taWhq2bdtmkffo168ffHx8sGLFCpPbaNu2bcOZM2cwduxYADXrWlVUVJjsGxoaCldXV+N+169fr3WVrHfv3gBwx1t0zs7OmDNnDs6cOYM5c+bUeaXtm2++waFDh4zvCwC//fabcXtpaSm++uorc0/bKDo6GgcOHMCqVauQn59vcmsOAB5//HHodDr861//qrVvdXV1reBG1JJwyQGiFmTgwIHw9PTEpEmT8NJLL0Emk+Hrr7+2qdtjb7/9Nn799VcMGjQI06ZNg06nw7JlyxAWFoZjx46ZdQytVot33323Vnvr1q0xffp0fPDBB5g8eTKGDh2KJ5980rjkQHBwMGbNmgUASE1NxfDhw/H444+jW7ducHBwwIYNG5Cbm2v8eP5XX32FTz/9FI888ghCQ0NRXFyMlStXws3NDWPGjLljja+++ipOnTqFjz76CLt37zauCJ6Tk4ONGzfi0KFD2LdvHwAgKioK7dq1w9/+9je8+uqrUCgUWLVqFby9vZGZmdmA0a0JRa+88gpeeeUVtG7dGpGRkSbbhw4diqlTp2LRokU4duwYoqKioFQqce7cOaxbtw4ff/yxyZpORC2KhJ/cIyILqG/Jge7du9fZf+/eveJPf/qTcHJyEgEBAeK1114TO3bsEADE7t27jf3qW3Kgro/gAxDz5883vq5vyYGYmJha+7Zv315MmjTJpC0+Pl706dNHqFQqERoaKv7v//5PvPzyy8LR0bGeUbhp0qRJAkCdX6GhocZ+a9euFX369BFqtVq0bt1aTJw4UVy+fNm4PT8/X8TExIguXbqIVq1aCXd3dzFgwADxww8/GPscOXJEPPnkk6Jdu3ZCrVYLHx8fMW7cOJGYmHjXOg3Wr18voqKiROvWrYWDg4Pw9/cX0dHRIiEhwaRfUlKSGDBggFCpVKJdu3ZiyZIl9S45MHbs2Du+56BBgwQA8fe//73ePl988YUIDw8XTk5OwtXVVfTo0UO89tprIisry+xzI2pu+Ow5IrILEyZMwKlTp3Du3DmpSyGiFopzmojI5pSXl5u8PnfuHLZu3Yphw4ZJUxAREQBeaSIim+Pv74/nnnsOHTp0wMWLF/HZZ5+hsrISR48exX333Sd1eUTUQnEiOBHZnFGjRuG7775DTk4O1Go1IiIisHDhQgYmIpIUrzQRERERmYFzmoiIiIjMwNBEREREZAbOaaqDXq9HVlYWXF1d633mExEREdkWIQSKi4sREBAAudzy14UYmuqQlZWFoKAgqcsgIiKiRrh06RLatm1r8eMyNNXB1dUVQM2gu7m5WfTYWq0Wv/76q/HRBGQdHHdpcNylwXG3Po65NG4fd41Gg6CgIOO/45bG0FQHwy05Nze3JglNzs7OcHNz439YVsRxlwbHXRocd+vjmEujvnFvqqk1nAhOREREZAaGJiIiIiIzMDQRERERmYGhiYiIiMgMDE1EREREZmBoIiIiIjIDQxMRERGRGRiaiIiIiMzA0ERERERkBoYmIiIiIjMwNBERERGZgaGJiIiIyAwMTVYkhECOpgL5FVJXQkRERA3F0GRF3xzMxODFv2FjBoediIjI3vBfbytq39oZAHC1QiZxJURERNRQDE1WFOLVCgCQXwHo9ELiaoiIiKghGJqsKMDDCUqFDNVChuwiTmwiIiKyJwxNVqSQy9Duxi26jGtlEldDREREDcHQZGXBbWpC08VrpRJXQkRERA3B0GRlhtDEK01ERET2haHJytozNBEREdklhiYr45UmIiIi+8TQZGXBbWqWHbh8vRzVOr3E1RAREZG5GJqszNdVDaVcoFovcPl6udTlEBERkZkYmqxMLpfBy7Hmz+n8BB0REZHdYGiSgLdjzWrgGfkMTURERPaCoUkC3jeuNDE0ERER2Q+GJgkYrjRdYGgiIiKyGwxNEvBxunF7jnOaiIiI7AZDkwQMt+euXC9HVTWXHSAiIrIHDE0ScFUCrVQK6AWQWcBFLomIiOwBQ5MEZLJbHqfCeU1ERER2gaFJIjcfp8LQREREZA8YmiRiuNLET9ARERHZB4YmiYTceAYdb88RERHZB4YmiQRzThMREZFdYWiSiOH2XFZRBSq0OomrISIiorthaJKIp7MSbo4OAICL17jsABERka1jaJKITCZDiFfNvKb0/BKJqyEiIqK7YWiSULAxNPFKExERka1jaJKQ4UoTJ4MTERHZPoYmCRlvz3GBSyIiIpvH0CShYK7VREREZDcYmiRkmNOUV1yJ0spqiashIiKiO2FokpC7kxKtW6kAAOm82kRERGTTGJokxgf3EhER2QeGJomFeLkA4LwmIiIiW8fQJLEQr5orTVyriYiIyLYxNEnMMBmct+eIiIhsG0OTxLjsABERkX1gaJKY4UrTtdIqFJVrJa6GiIiI6sPQJDEXtQO8XdUAeLWJiIjIljE02YAQzmsiIiKyeQxNNiDkxrwmLnBJRERkuxiabIDxE3QMTURERDaLockG3FyriaGJiIjIVjE02QDDlab0/FIIISSuhoiIiOrC0GQD2reuCU2aimpcL+OyA0RERLaIockGOKkUCHB3BMBbdERERLZK8tC0fPlyBAcHw9HREQMGDMChQ4fu2L+wsBAxMTHw9/eHWq1Gp06dsHXrVuN2nU6Ht956CyEhIXByckJoaCj+9a9/2fxtL04GJyIism0OUr752rVrMXv2bKxYsQIDBgzA0qVLMXLkSKSkpMDHx6dW/6qqKowYMQI+Pj5Yv349AgMDcfHiRXh4eBj7fPDBB/jss8/w1VdfoXv37khMTMTkyZPh7u6Ol156yYpn1zDBXq2w7/w1rtVERERkoyQNTUuWLMGUKVMwefJkAMCKFSuwZcsWrFq1Cq+//nqt/qtWrUJBQQH27dsHpVIJAAgODjbps2/fPowfPx5jx441bv/uu+/uegVLaoa1mi7wShMREZFNkuz2XFVVFZKSkhAZGXmzGLkckZGR2L9/f537/Pzzz4iIiEBMTAx8fX0RFhaGhQsXQqfTGfsMHDgQ8fHxSE1NBQAcP34cf/zxB0aPHt20J3SPeHuOiIjItkl2pSk/Px86nQ6+vr4m7b6+vjh79myd+1y4cAG7du3CxIkTsXXrVqSlpWH69OnQarWYP38+AOD111+HRqNBly5doFAooNPp8N5772HixIn11lJZWYnKykrja41GAwDQarXQai37aTbD8W4/blt3FYCa0FRVVQWZTGbR923p6ht3alocd2lw3K2PYy6N28e9qcdf0ttzDaXX6+Hj44MvvvgCCoUC4eHhuHLlChYvXmwMTT/88AO+/fZbrFmzBt27d8exY8cwc+ZMBAQEYNKkSXUed9GiRViwYEGt9l9//RXOzs5Nci5xcXEmr6v1gAwKlFbpsHbTNripmuRtW7zbx52sg+MuDY679XHMpWEY97KysiZ9H8lCk5eXFxQKBXJzc03ac3Nz4efnV+c+/v7+UCqVUCgUxrauXbsiJycHVVVVUKlUePXVV/H666/jiSeeAAD06NEDFy9exKJFi+oNTXPnzsXs2bONrzUaDYKCghAVFQU3N7d7PVUTWq0WcXFxGDFihHFelsF/Un/H5evlCO0dgf7BnhZ935buTuNOTYfjLg2Ou/VxzKVx+7gb7hQ1FclCk0qlQnh4OOLj4zFhwgQANVeS4uPjERsbW+c+gwYNwpo1a6DX6yGX10zHSk1Nhb+/P1SqmkszZWVlxm0GCoUCer2+3lrUajXUanWtdqVS2WQ//HUdO8SrFS5fL8flwkoM5H90TaIp/06pfhx3aXDcrY9jLg3DuDf12Eu6TtPs2bOxcuVKfPXVVzhz5gymTZuG0tJS46fpnn32WcydO9fYf9q0aSgoKMCMGTOQmpqKLVu2YOHChYiJiTH2eeihh/Dee+9hy5YtyMjIwIYNG7BkyRI88sgjVj+/hgrx4ifoiIiIbJWkc5qio6Nx9epVzJs3Dzk5Oejduze2b99unByemZlpctUoKCgIO3bswKxZs9CzZ08EBgZixowZmDNnjrHPJ598grfeegvTp09HXl4eAgICMHXqVMybN8/q59dQwW34CToiIiJbJflE8NjY2HpvxyUkJNRqi4iIwIEDB+o9nqurK5YuXYqlS5daqELrMVxp4gKXREREtkfyx6jQTbeGJr3eth/7QkRE1NIwNNmQtp5OcJDLUKHVI7e4QupyiIiI6BYMTTbEQSFHUOuadaHSr/IWHRERkS1haLIxwW1uhCbOayIiIrIpDE02hs+gIyIisk0MTTamw43QlJ7ftEvBExERUcMwNNmYYC47QEREZJMYmmyMYYHLzGtl0HHZASIiIpvB0GRjAjycoFLIUaXTI6uwXOpyiIiI6AaGJhujkMvQzvAJOk4GJyIishkMTTbI+Aw6zmsiIiKyGQxNNqiDt+ETdAxNREREtoKhyQYZrzQxNBEREdkMhiYbFOzFOU1ERES2hqHJBoXcWKvp0vVyaHV6iashIiIigKHJJvm6OsJRKYdOL3D5OpcdICIisgUMTTZILpdxXhMREZGNYWiyUSFe/AQdERGRLWFoslF8Bh0REZFtYWiyUSFteKWJiIjIljA02ahg3p4jIiKyKQxNNsqwVlNWYTkqq3USV0NEREQMTTbK20WNVioF9AK4VFAmdTlEREQtHkOTjZLJZAgxPoOOoYmIiEhqDE02jGs1ERER2Q6GJhtmWKvpAkMTERGR5BiabBivNBEREdkOhiYbxgUuiYiIbAdDkw0z3J7LLqpAeRWXHSAiIpISQ5MN83RWwt1JCQC4WMCrTURERFJiaLJhMpns5srgVxmaiIiIpMTQZONC2tSsDJ7OeU1ERESSYmiyccbJ4PwEHRERkaQYmmxciDE0cVVwIiIiKTE02TjDWk28PUdERCQthiYbZ7g9d7W4EiWV1RJXQ0RE1HIxNNk4dycl2rRSAeC8JiIiIikxNNkB47IDDE1ERESSYWiyA3wGHRERkfQYmuxAiBfXaiIiIpIaQ5Md4FpNRERE0mNosgPGtZquca0mIiIiqTA02QHDnKaC0ioUlWklroaIiKhlYmiyA63UDvBxVQPgvCYiIiKpMDTZCc5rIiIikhZDk50IacO1moiIiKTE0GQnjFeaeHuOiIhIEgxNdiKEq4ITERFJiqHJTtwamoQQEldDRETU8jA02Yn2bWpWBS+uqEZBaZXE1RAREbU8DE12wlGpQIC7IwDOayIiIpICQ5MdCTbeouPK4ERERNbG0GRHuFYTERGRdBia7EgHfoKOiIhIMgxNdiSYC1wSERFJhqHJjty6wCWXHSAiIrIuhiY70q61M+QyoKxKh6vFlVKXQ0RE1KIwNNkRlYMcgZ5OAHiLjoiIyNoYmuyMYV4T12oiIiKyLoYmO2P4BN0FXmkiIiKyKoYmO8O1moiIiKTB0GRnboYmrgpORERkTQxNdibkljlNej2XHSAiIrIWhiY709bTCQ5yGSqr9cjRVEhdDhERUYvB0GRnHBRyBLV2BsBlB4iIiKxJ8tC0fPlyBAcHw9HREQMGDMChQ4fu2L+wsBAxMTHw9/eHWq1Gp06dsHXrVpM+V65cwdNPP402bdrAyckJPXr0QGJiYlOehlWF8Bl0REREVucg5ZuvXbsWs2fPxooVKzBgwAAsXboUI0eOREpKCnx8fGr1r6qqwogRI+Dj44P169cjMDAQFy9ehIeHh7HP9evXMWjQIDzwwAPYtm0bvL29ce7cOXh6elrxzJqWca0mhiYiIiKrkTQ0LVmyBFOmTMHkyZMBACtWrMCWLVuwatUqvP7667X6r1q1CgUFBdi3bx+USiUAIDg42KTPBx98gKCgIHz55ZfGtpCQkKY7CQmEeNXcnuMCl0RERNYj2e25qqoqJCUlITIy8mYxcjkiIyOxf//+Ovf5+eefERERgZiYGPj6+iIsLAwLFy6ETqcz6dOvXz/89a9/hY+PD/r06YOVK1c2+flYUzBvzxEREVmdZFea8vPzodPp4Ovra9Lu6+uLs2fP1rnPhQsXsGvXLkycOBFbt25FWloapk+fDq1Wi/nz5xv7fPbZZ5g9ezbeeOMNHD58GC+99BJUKhUmTZpU53ErKytRWXnzAbgajQYAoNVqodVqLXG6Robj3ctx27qrAQCZBWWoqKyCQi6zSG3NmSXGnRqO4y4Njrv1ccylcfu4N/X4y4QQkiz2k5WVhcDAQOzbtw8RERHG9tdeew179uzBwYMHa+3TqVMnVFRUID09HQqFAkDNLb7FixcjOzsbAKBSqdCvXz/s27fPuN9LL72Ew4cP13sF6+2338aCBQtqta9ZswbOzs73dJ5NQS+AVw4qoBMyvNWnGl6OUldEREQkvbKyMjz11FMoKiqCm5ubxY8v2ZUmLy8vKBQK5ObmmrTn5ubCz8+vzn38/f2hVCqNgQkAunbtipycHFRVVUGlUsHf3x/dunUz2a9r16748ccf661l7ty5mD17tvG1RqNBUFAQoqKiLD7oWq0WcXFxGDFihHFeVmMsP78XaVdLEdLjfgy+z8uCFTZPlhp3ahiOuzQ47tbHMZfG7eNuuFPUVCQLTSqVCuHh4YiPj8eECRMAAHq9HvHx8YiNja1zn0GDBmHNmjXQ6/WQy2umY6WmpsLf3x8qlcrYJyUlxWS/1NRUtG/fvt5a1Go11Gp1rXalUtlkP/z3euwQbxekXS3FpcJK/gfaAE35d0r147hLg+NufRxzaRjGvanHXtJ1mmbPno2VK1fiq6++wpkzZzBt2jSUlpYaP0337LPPYu7cucb+06ZNQ0FBAWbMmIHU1FRs2bIFCxcuRExMjLHPrFmzcODAASxcuBBpaWlYs2YNvvjiC5M+zUGotwsA4GxO06ZqIiIiqiHpkgPR0dG4evUq5s2bh5ycHPTu3Rvbt283Tg7PzMw0XlECgKCgIOzYsQOzZs1Cz549ERgYiBkzZmDOnDnGPv3798eGDRswd+5cvPPOOwgJCcHSpUsxceJEq59fU+rbzgMAkJhxXdpCiIiIWghJQxMAxMbG1ns7LiEhoVZbREQEDhw4cMdjjhs3DuPGjbNEeTYrvH3NYp3n8kpQWFYFD2eVxBURERE1b426Pbd9+3b88ccfxtfLly9H79698dRTT+H6dV75sIY2Lmp08K5ZrynpIseciIioqTUqNL366qvGGeonT57Eyy+/jDFjxiA9Pd3kU2jUtPrduNp0mLfoiIiImlyjbs+lp6cbP9b/448/Yty4cVi4cCGOHDmCMWPGWLRAql+/4Nb4IfEyki4WSF0KERFRs9eoK00qlQplZWUAgJ07dyIqKgoA0Lp16yZfI4Fu6h/cGgBw/FIRKrS6u/QmIiKie9GoK01//vOfMXv2bAwaNAiHDh3C2rVrAdSsh9S2bVuLFkj1C27jjDatVLhWWoXkK0XodyNEERERkeU16krTsmXL4ODggPXr1+Ozzz5DYGAgAGDbtm0YNWqURQuk+slkMvQLrpnXlMjJ4ERERE2qUVea2rVrh82bN9dq/89//nPPBVHD9GvfGjtO5SIxowAYGip1OURERM1Wo640HTlyBCdPnjS+3rRpEyZMmIA33ngDVVVVFiuO7s5wpSnp4nXo9ZI8e5mIiKhFaFRomjp1KlJTUwEAFy5cwBNPPAFnZ2esW7cOr732mkULpDvrHuAOR6Uc18u0uJBfInU5REREzVajQlNqaip69+4NAFi3bh2GDBmCNWvWYPXq1fjxxx8tWR/dhcpBjl5tPQBwvSYiIqKm1KjQJISAXq8HULPkgGFtpqCgIOTn51uuOjKLYekBPoeOiIio6TQqNPXr1w/vvvsuvv76a+zZswdjx44FULPopeFhu2Q94cZP0HGRSyIioqbSqNC0dOlSHDlyBLGxsXjzzTfRsWNHAMD69esxcOBAixZId9e3nSdkMuDitTLkFVdIXQ4REVGz1KglB3r27Gny6TmDxYsXQ6FQ3HNR1DDuTkp09nXF2ZxiJGVcx+ge/lKXRERE1Ow0KjQZJCUl4cyZMwCAbt26oW/fvhYpihquX7AnzuYUI/EiQxMREVFTaFRoysvLQ3R0NPbs2QMPDw8AQGFhIR544AF8//338Pb2tmSNZIb+wa3xzYHMmkUuiYiIyOIaNafpxRdfRElJCU6dOoWCggIUFBQgOTkZGo0GL730kqVrJDOEt6+ZDJ6cpUFZVbXE1RARETU/jQpN27dvx6effoquXbsa27p164bly5dj27ZtFiuOzBfo4QR/d0fo9ALHLhVKXQ4REVGz06jQpNfroVQqa7UrlUrj+k1kXTKZzHi1ies1ERERWV6jQtODDz6IGTNmICsry9h25coVzJo1C8OHD7dYcdQwxkUuLzI0ERERWVqjQtOyZcug0WgQHByM0NBQhIaGIiQkBBqNBv/9738tXSOZyfDw3iMXr0PHh/cSERFZVKM+PRcUFIQjR45g586dOHv2LACga9euiIyMtGhx1DBd/NzgonZASWU1zuZo0D3AXeqSiIiImo1Gr9Mkk8kwYsQIjBgxwth29uxZPPzww0hNTbVIcdQwCrkMfdp54Pdz+Ui6eJ2hiYiIyIIadXuuPpWVlTh//rwlD0kN1K99zbymw5wMTkREZFEWDU0kvf435jUlcZFLIiIii2JoamZ6t/OAQi5DVlEFrhSWS10OERFRs8HQ1Mw4qxzQPcANAPhIFSIiIgtq0ERwT09PyGSyerdXV/PxHbagX/vWOHG5CIkZ1zG+d6DU5RARETULDQpNS5cubaIyyJL6BXti1d50HOaVJiIiIotpUGiaNGlSU9VBFtTvxuNUUnKLoanQws2x9iNviIiIqGE4p6kZ8nFzRLvWzhCiZnVwIiIiuncMTc2U4ZEqSQxNREREFsHQ1EwZHt7LeU1ERESWwdDUTBnmNR27VAitTi9xNURERPavQaFp8ODB+PDDD/lsOTsQ6u0CD2clKrR6nMrSSF0OERGR3WtQaJoyZQr279+P8PBwdO3aFXPmzMHevXshhGiq+qiR5HIZwtvVXG3iIpdERET3rkGh6dlnn8WPP/6I/Px8fPTRRygsLMRf//pX+Pn54fnnn8fGjRtRXs5Hd9iKfjfmNSXy4b1ERET3rFFzmtRqNcaMGYPPP/8cWVlZ+Pnnn+Hv74+33noLbdq0wbhx47B3715L10oNZHh4b+LFAl4NJCIiukcWmQg+YMAAvPfeezh58iROnjyJ4cOHIzs72xKHpnsQFugOlUKO/JIqZFwrk7ocIiIiu9agFcHNERoailmzZln6sNQIjkoFerZ1R+LF60jMKECIVyupSyIiIrJbXHKgmQs33KLjvCYiIqJ7wtDUzPVvf2My+EV+go6IiOheMDQ1c+E3Frk8f7UUBaVVEldDRERkvxoVmt555x2UldWeWFxeXo533nnnnosiy/FspUJHHxcAfA4dERHRvWhUaFqwYAFKSkpqtZeVlWHBggX3XBRZlnHpAS5ySURE1GiNCk1CCMhkslrtx48fR+vWre+5KLKs8PZ8eC8REdG9atCSA56enpDJZJDJZOjUqZNJcNLpdCgpKcELL7xg8SLp3hiuNJ28UoQKrQ6OSoXEFREREdmfBoWmpUuXQgiB559/HgsWLIC7u7txm0qlQnBwMCIiIixeJN2bdq2d4eWiRn5JJU5cLsL9IbwaSERE1FANCk2TJk0CAISEhGDQoEFwcLD42pjUBGQyGfoHe2Jbcg4SLxYwNBERETVCo+Y0ubq64syZM8bXmzZtwoQJE/DGG2+gqoofa7dFfHgvERHRvWlUaJo6dSpSU1MBABcuXEB0dDScnZ2xbt06vPbaaxYtkCyjX/ubn6DT6/nwXiIiooZqVGhKTU1F7969AQDr1q3D0KFDsWbNGqxevRo//vijJesjC+kW4AYnpQKaimqkXa29XAQRERHdWaOXHNDr9QCAnTt3YsyYMQCAoKAg5OfnW646shilQo7eQR4AuPQAERFRYzQqNPXr1w/vvvsuvv76a+zZswdjx44FAKSnp8PX19eiBZLlGJYeSOK8JiIiogZrVGhaunQpjhw5gtjYWLz55pvo2LEjAGD9+vUYOHCgRQskyzFMBj/Mh/cSERE1WKPWDOjZsydOnjxZq33x4sVQKLhwoq3q084DchlwqaAcuZoK+Lo5Sl0SERGR3binhZaSkpKMSw9069YNffv2tUhR1DRcHZXo4ueG09kaJGZcx9ie/lKXREREZDcaFZry8vIQHR2NPXv2wMPDAwBQWFiIBx54AN9//z28vb0tWSNZUL9gT5zO1uBwRgFDExERUQM0ak7Tiy++iJKSEpw6dQoFBQUoKChAcnIyNBoNXnrpJUvXSBZkmNeUdJGTwYmIiBqiUVeatm/fjp07d6Jr167Gtm7dumH58uWIioqyWHFkeYZP0J3O1qC0shqt1HwUDhERkTkadaVJr9dDqVTWalcqlcb1m8g2+bs7IdDDCTq9wLFLhVKXQ0REZDcaFZoefPBBzJgxA1lZWca2K1euYNasWRg+fLjFiqOm0e/G1SYucklERGS+RoWmZcuWQaPRIDg4GKGhoQgNDUVISAg0Gg0++eQTS9dIFnbzOXSc10RERGSuRk1oCQoKwpEjR7Bz506cPXsWANC1a1dERkZatDhqGobJ4Eczr6Nap4eDolHZmYiIqEVp9CxgmUyGESNGYMSIEZash6ygk68rXNUOKK6sxtmcYoQFuktdEhERkc1r0CWGXbt2oVu3btBoNLW2FRUVoXv37vj9998tVhw1DYVchr7GW3Sc10RERGSOBoWmpUuXYsqUKXBzc6u1zd3dHVOnTsWSJUsaXMTy5csRHBwMR0dHDBgwAIcOHbpj/8LCQsTExMDf3x9qtRqdOnXC1q1b6+z7/vvvQyaTYebMmQ2uqzkzLD1wmOs1ERERmaVBoen48eMYNWpUvdujoqKQlJTUoALWrl2L2bNnY/78+Thy5Ah69eqFkSNHIi8vr87+VVVVGDFiBDIyMrB+/XqkpKRg5cqVCAwMrNX38OHD+Pzzz9GzZ88G1dQShLevmdeUmFEAIYTE1RAREdm+BoWm3NzcOtdnMnBwcMDVq1cbVMCSJUswZcoUTJ48Gd26dcOKFSvg7OyMVatW1dl/1apVKCgowMaNGzFo0CAEBwdj6NCh6NWrl0m/kpISTJw4EStXroSnp2eDamoJegd5wEEuQ66mEpevl0tdDhERkc1r0ETwwMBAJCcno2PHjnVuP3HiBPz9zX+eWVVVFZKSkjB37lxjm1wuR2RkJPbv31/nPj///DMiIiIQExODTZs2wdvbG0899RTmzJkDhUJh7BcTE4OxY8ciMjIS77777h3rqKysRGVlpfG1Yc6WVquFVqs1+3zMYTiepY/bUA4yoFuAK05c1uDg+avwcw2QtJ6mZivj3tJw3KXBcbc+jrk0bh/3ph7/BoWmMWPG4K233sKoUaPg6Ohosq28vBzz58/HuHHjzD5efn4+dDodfH19Tdp9fX2NSxnc7sKFC9i1axcmTpyIrVu3Ii0tDdOnT4dWq8X8+fMBAN9//z2OHDmCw4cPm1XHokWLsGDBglrtv/76K5ydnc0+n4aIi4trkuM2RGudHIAcG/44AWXWManLsQpbGPeWiOMuDY679XHMpWEY97KysiZ9nwaFpn/+85/46aef0KlTJ8TGxqJz584AgLNnz2L58uXQ6XR48803m6RQA71eDx8fH3zxxRdQKBQIDw/HlStXsHjxYsyfPx+XLl3CjBkzEBcXVyvY1Wfu3LmYPXu28bVGo0FQUBCioqLqnPR+L7RaLeLi4jBixIg73uq0BofTuUj47jiuCjeMGTNQ0lqami2Ne0vCcZcGx936OObSuH3c6/p0vyU1KDT5+vpi3759mDZtGubOnWucQCyTyTBy5EgsX7681lWjO/Hy8oJCoUBubq5Je25uLvz8/Orcx9/fH0ql0uRWXNeuXZGTk2O83ZeXl4e+ffsat+t0Ovz2229YtmwZKisrTfYFALVaDbVaXeu9lEplk/3wN+WxzXV/B28AQGpeCcq0gLtz8/8P3RbGvSXiuEuD4259HHNpGMa9qce+wUtBt2/fHlu3bkV+fj4OHjyIAwcOID8/H1u3bkVISEiDjqVSqRAeHo74+Hhjm16vR3x8PCIiIurcZ9CgQUhLSzN5MHBqair8/f2hUqkwfPhwnDx5EseOHTN+9evXDxMnTsSxY8dqBaaWzNtVjRCvVgCAI5lceoCIiOhOGr0iuKenJ/r373/PBcyePRuTJk1Cv379cP/992Pp0qUoLS3F5MmTAQDPPvssAgMDsWjRIgDAtGnTsGzZMsyYMQMvvvgizp07h4ULF+Kll14CALi6uiIsLMzkPVq1aoU2bdrUaicgvL0n0vNLcTijAA908ZG6HCIiIpvV6NBkKdHR0bh69SrmzZuHnJwc9O7dG9u3bzfe5svMzIRcfvOCWFBQEHbs2IFZs2ahZ8+eCAwMxIwZMzBnzhypTsGu9Q/2xPqky0jkIpdERER3JHloAoDY2FjExsbWuS0hIaFWW0REBA4cOGD28es6BtUwPLz3+KVCVFXroXLgw3uJiIjqwn8hW7gOXq3QupUKldV6nLxSKHU5RERENouhqYWTyWQYGNoGALD5RLbE1RAREdkuhibCY33bAgA2HctCVbX+Lr2JiIhaJoYmwuD7vODtqkZBaRUSUup+UDIREVFLx9BEcFDI8UifQADA+qTLEldDRERkmxiaCMDNW3S7zubhWknlXXoTERG1PAxNBADo7OeKHoHuqNYL/Hw8S+pyiIiIbA5DExn9JbzmahNv0REREdXG0ERGD/cKgFIhw6ksDc5kN+2ToomIiOwNQxMZebZSYXiXmsfX/MirTURERCYYmsjEYzdu0W08lgWtjms2ERERGTA0kYlhnb3RppUK+SWV+C31qtTlEBER2QyGJjKhVMgx4caaTT8e4S06IiIiA4YmqsWwZtPO03m4XlolcTVERES2gaGJaukW4IZu/m6o0unxywmu2URERAQwNFE9DBPC+Sk6IiKiGgxNVKfxvQPgIJfh+OUinMstlrocIiIiyTE0UZ28XNR4oIsPAGA9J4QTERExNFH9DBPCNxy5gmqu2URERC0cQxPV68EuPvB0ViKvuBK/p+VLXQ4REZGkGJqoXioHOcb3vrFmEyeEExFRC8fQRHf0lxufovv1dC6KyrQSV0NERCQdhia6o+4Bbujs64qqaj02n+SaTURE1HIxNNEdyWQy49Wm9bxFR0RELRhDE93V+D4BUMhlOJpZiPNXS6Quh4iISBIMTXRXPq6OGNrJGwAnhBMRUcvF0ERmMdyi++nIFej0QuJqiIiIrI+hicwyvKsP3J2UyNFUYN95rtlEREQtD0MTmUXtoMDDvQIAcEI4ERG1TAxNZDbDLbodp3KgqeCaTURE1LIwNJHZerZ1R0cfF1Ro9dh6IlvqcoiIiKyKoYnMxjWbiIioJWNoogZ5pE8g5DIg8eJ1ZOSXSl0OERGR1TA0UYP4ujli8H031mw6wqtNRETUcjA0UYPdumaTnms2ERFRC8HQRA02opsvXB0dcKWwHAcuXJO6HCIiIqtgaKIGc1Qq8BDXbCIiohaGoYka5bG+NbfotiXnoKSyWuJqiIiImh5DEzVK33Ye6ODVCuVaHbae5JpNRETU/DE0UaPIZDI8dmNC+I+8RUdERC0AQxM12iN9AiGTAQfTC3CpoEzqcoiIiJoUQxM1WoCHE/7c0QsA12wiIqLmj6GJ7olhQviPRy5zzSYiImrWGJronozs7gcXtQMuFZTjUEaB1OUQERE1GYYmuidOKgXG9vAHwAnhRETUvDE00T37S7+aW3RbT2ajrIprNhERUfPE0ET3rF97T7Rv44zSKh22J+dIXQ4REVGTYGiieyaTyYwTwvlYFSIiaq4YmsgiHu0bCADYd/4aLl/nmk1ERNT8MDSRRbT1dEZEhzYAgA1HrkhcDRERkeUxNJHF/OXGY1XWc80mIiJqhhiayGJG96hZs+nitTKuEE5ERM0OQxNZjLPKAS8+2BEA8P62sygq00pcERERkeUwNJFFPf/nENzn44JrpVVY/OtZqcshIiKyGIYmsiilQo53xocBAL49mIkTlwulLYiIiMhCGJrI4iJC22BC7wAIAby1MRk6TgonIqJmgKGJmsQbY7vCVe2A45eL8N2hTKnLISIiumcMTdQkfFwdMTuqEwBg8Y4UXCuplLgiIiKie8PQRE3mmT+1Rzd/NxSVa/H+Nk4KJyIi+8bQRE3GQSHHvybUTApfl3QZiRkFEldERETUeAxN1KTC23vi8X41K4X/c2MyqnV6iSsiIiJqHIYmanJzRnWBu5MSZ3OK8b/9F6Uuh4iIqFEYmqjJtXFR47VRnQEAS+JSkaepkLgiIiKihmNoIqt4on879AryQEllNd7bekbqcoiIiBqMoYmsQiGX4d3xYZDJgE3HsrDvfL7UJRERETWITYSm5cuXIzg4GI6OjhgwYAAOHTp0x/6FhYWIiYmBv78/1Go1OnXqhK1btxq3L1q0CP3794erqyt8fHwwYcIEpKSkNPVp0F30aOuOpwe0BwDM23QKVdWcFE5ERPZD8tC0du1azJ49G/Pnz8eRI0fQq1cvjBw5Enl5eXX2r6qqwogRI5CRkYH169cjJSUFK1euRGBgoLHPnj17EBMTgwMHDiAuLg5arRZRUVEoLS211mlRPV6J6ow2rVRIyyvBqr3pUpdDRERkNgepC1iyZAmmTJmCyZMnAwBWrFiBLVu2YNWqVXj99ddr9V+1ahUKCgqwb98+KJVKAEBwcLBJn+3bt5u8Xr16NXx8fJCUlIQhQ4Y0zYmQWdydlZg7piteWXccH+88h4d6BSDQw0nqsoiIiO5K0tBUVVWFpKQkzJ0719gml8sRGRmJ/fv317nPzz//jIiICMTExGDTpk3w9vbGU089hTlz5kChUNS5T1FREQCgdevWdW6vrKxEZeXNx3xoNBoAgFarhVarbdS51cdwPEsf15483MMH3x/yQOLFQiz4ORnLn+zd5O/JcZcGx10aHHfr45hL4/Zxb+rxlzQ05efnQ6fTwdfX16Td19cXZ8/W/diNCxcuYNeuXZg4cSK2bt2KtLQ0TJ8+HVqtFvPnz6/VX6/XY+bMmRg0aBDCwsLqPOaiRYuwYMGCWu2//vornJ2dG3FmdxcXF9ckx7UXw92BI1Dg19N5+OjbbejqKazyvi193KXCcZcGx936OObSMIx7WVlZk76P5LfnGkqv18PHxwdffPEFFAoFwsPDceXKFSxevLjO0BQTE4Pk5GT88ccf9R5z7ty5mD17tvG1RqNBUFAQoqKi4ObmZtH6tVot4uLiMGLECOPtxZYqzyUFX+67iK15Loh9fCDUyrqvFFoCx10aHHdpcNytj2MujdvH3XCnqKlIGpq8vLygUCiQm5tr0p6bmws/P7869/H394dSqTS5Fde1a1fk5OSgqqoKKpXK2B4bG4vNmzfjt99+Q9u2beutQ61WQ61W12pXKpVN9sPflMe2F7OjOmPLyRxkFpTj/+27hBmR9zX5e3LcpcFxlwbH3fo45tIwjHtTj72kn55TqVQIDw9HfHy8sU2v1yM+Ph4RERF17jNo0CCkpaVBr7/5cfXU1FT4+/sbA5MQArGxsdiwYQN27dqFkJCQpj0RahRXRyX+Oa4bAODThDRkXmvay6pERET3QvIlB2bPno2VK1fiq6++wpkzZzBt2jSUlpYaP0337LPPmkwUnzZtGgoKCjBjxgykpqZiy5YtWLhwIWJiYox9YmJi8M0332DNmjVwdXVFTk4OcnJyUF5ebvXzozt7qKc/Boa2QWW1Hm//cgpCWGduExERUUNJPqcpOjoaV69exbx585CTk4PevXtj+/btxsnhmZmZkMtvZrugoCDs2LEDs2bNQs+ePREYGIgZM2Zgzpw5xj6fffYZAGDYsGEm7/Xll1/iueeea/JzIvPJZDK8Mz4Moz/+DbvO5iHudC6iutd9a5aIiEhKkocmoGbuUWxsbJ3bEhISarVFRETgwIED9R6PVyvsS0cfF/x9cAd8lnAeC345jcH3ecNJ1XSTwomIiBpD8ttzRADw4oMdEejhhCuF5Vi2+5zU5RAREdXC0EQ2wVnlgHkP1UwK/+K3Czh/tUTiioiIiEwxNJHNiOrmiwc6e0OrE5i3KZm3WYmIyKYwNJHNkMlkePvh7lA5yLE37Ro2n8iWuiQiIiIjhiayKe3btML0YaEAgHe3nEZJZbXEFREREdVgaCKb88LQULRv44xcTSU+3JEidTlEREQAGJrIBjkqFXj74e4AgNX7MrB8d5rEFRERETE0kY16oLMPXh3ZGQCweEcKgxMREUmOoYlsVswDHfFKVCcADE5ERCQ9hiayabEP3mdyxWnZLi58SURE0mBoIpsX80BHY3D68NdUBiciIpIEQxPZhduD0yfxDE5ERGRdDE1kN24NTh/FMTgREZF1MTSRXYl5oCNeG3UzOP2XwYmIiKyEoYnszvRhN4PTEgYnIiKyEoYmskvTh3XEnFFdADA4ERGRdTA0kd2aNizUJDh9vJPBiYiImg5DE9m1acNC8fromuD0n50MTkRE1HQYmsjuvTDUNDgt3ZkqcUVERNQcMTRRs/DC0FDMvRGclu48x+BEREQWx9BEzcbU24LTf+IYnIiIyHIYmqhZmTo0FG+MqQlOH8czOBERkeU4SF0AkaX9Y0goAGDh1rP4+MZSBLHDQqQsiYiImgGGJmqW/jEkFDLI8N7WM/g4/hz0eh06Sl0UERHZNd6eo2ZrypAOeHNMVwDAJ7svYE2aHAWlVRJXRURE9oqhiZq1KUM64J9ja4LTwatyRH38B77enwGdXkhcGRER2RuGJmr2/j64A9b8rT8CnAWKyqvx1qZTGPfJHzicUSB1aUREZEcYmqhF6B/siVd66jB/XBe4OTrgTLYGf12xHzO/P4pcTYXU5RERkR1gaKIWQyEDnh7QDrtfGYYn7w+CTAZsPJaFBz9MwOd7zqOqWi91iUREZMMYmqjFaeOixqJHe2JTzCD0aeeB0iodFm07i1Ef/4Y9qVelLo+IiGwUQxO1WD3beuDHFwbiw7/2gpeLGheulmLSqkOY8r9EXCook7o8IiKyMQxN1KLJ5TL8Jbwtdr0yFH/7cwgUchniTudi+JI9WBKXivIqndQlEhGRjWBoIgLg5qjEW+O6YfuMwRjUsQ2qqvX4b/w5RC7Zg+3J2RCCSxQQEbV0DE1Et7jP1xXf/G0APp3YFwHujrhSWI4XvjmCZ/7fIaTlFUtdHhERSYihieg2MpkMY3r4I/7lYXjpwY5QOcjxR1o+Ri39He9uPo3iCq3UJRIRkQQYmojq4aRSYHZUZ+ycNRSRXX1RrRf4vz/S8cCHe/DB9rM4naXhbTsiohaED+wluot2bZzxf5P6ISElDwt+OY30/FJ8lnAenyWcR6h3K4zrGYCHegWgo4+L1KUSEVETYmgiMtOwzj4YGOqFuNO52HwiC/Fn83D+aik+jj+Hj+PPoau/G8b19MdDPQPQro2z1OUSEZGFMTQRNYDKQY6xPf0xtqc/iiu02HkmF78cz8bv567iTLYGZ7I1WLwjBb3auuOhXgEY29Mf/u5OUpdNREQWwNBE1Eiujko80qctHunTFoVlVdhxKge/HM/GvvP5OH65CMcvF+HdLWfQP9gTD/UKwOgwf3i7qqUum4iIGomhicgCPJxViO7fDtH92+FqcSW2J2fjl+PZOJRRgMMZ13E44zre/vkUIkLb4KGeARgV5gcPZ5XUZRMRUQMwNBFZmLerGs9EBOOZiGBkF5Vjy4ls/HIiG8cvFWJv2jXsTbuGf25MxuD7vDA6zB+9gjwQ6t0KDgp+mJWIyJYxNBE1IX93J/x9cAf8fXAHZF4rwy8nsrD5RDbOZGuwO+UqdqfUPCBY7SBHV383hAW6ISzAHWGB7rjP1wVqB4XEZ0BERAYMTURW0q6NM2Ie6IiYBzoiLa/YOP/pdJYGpVU6HLtUiGOXCo39lQoZOvm63ghRbuge6I6ufm5wUjFIERFJgaGJSAIdfVwxa4QrZo3oBL1eIONaKZKzNDh1pQinsjQ4eaUIReVanMrS4FSWBmsTa/aTy4COPi4IC3BH90B3hAW4oVuAG1wdldKeEBFRC8DQRCQxuVyGDt4u6ODtgod7BQAAhBC4UliO5CsanMoqQvKVIpy8okF+SSVSc0uQmluCn45eMR4jxKsVOvq4wN/dEX7ujjXf3ZyMrx2VvDpFRHSvGJqIbJBMJkNbT2e09XTGqDA/Y3uepgLJWUVIvqJB8o2rUlcKy5GeX4r0/NJ6j+fprISf+80Q5e9mCFdOxpDVSs1fB0REd8LfkkR2xMfNEQ+6OeLBLr7GtuulVUjOKsLFa2XIKapAdlEFcjTlyC6qQHZhBcq1Olwv0+J6mRZnsjX1HtvV0eFGqHJCa2clXB2VcHNyqPlu8mcH4zY3RyXUDnLIZDJrnD4RkaQYmojsnGcrFQbf543B99XeJoSApqL6RpgqR05RBbKKKpBTVBOqcm58FVdWo7iiGsUVNbf+GkKlkMPV0QFuTsqa7443v7dSyXE5U46MhAtodSNgqZUKOCoVUDvITb47KuVQO9T+rpAzkBGRbWBoImrGZDIZ3J2UcHdSorOfa739iiu0yNXUXKXKLqpAUZkWmgotiiuqoSnXQlNRbfK6uEKL4spqCAFU6fS4VlqFa6VV9Rxdjl+vpDX6HJQKGRwdFFA5yOGgkMFBLodSIYNCLoNSUdOmkMuhlMvgoKhpU8jr6Hdju4O8ZrtCLoNcVjOnTC6TQSGT3fgzbvmzDAo5IJcZ/lx7H5msZpzlMtT8Gbe13Xhdk/0M/WSQAZDLa7ZDVvMeMtQco6bnjePUvKjdduPv1/D65sU+GXS6amQUA8cuFUKpvPkhgZv73Twebmur6/Wt/Wptu32/O/S9tYbb1X+xsu4Nd7q4Wf971HOs+g9lturqalwtBy4WlEHpcOd/Wm8fo3r72eD/L1iyJielAm1c7OspCQxNRARXx5rbcR196g9Wt9PrBUqrqqGpqEZxhRaa8hvfbwlXhWVVSEm7AL/AIGh1AhVaPSqrdSbfK6p1qLzxuvLGa61OGN9HqxPQ6qqByqY48+bMAf9JPiR1ES2MA9499ofURdiNh3sF4L9P9pG6jAZhaCKiRpHLZcawBdT9UGKtVoutujSMGdPd5IrH3ej0wiREGUJWtU6gWi9QrdPf+C6g1euh0wlU6/XQ6gR0egGtYbuh7y37afUCOr0eelET/HR6UfNnIaAXhtcCej2gE4Y/C+gMffSGfjf3EQIQqLkdKm70M36/pV3g5jZxY9vNvjXnLoS48f3Ga9zc17Bd1GyA4Zux7cZ+eiFQXlYGJycnQCYzHgu3HOOWQ9R6P9PXt+9r8uoO2+ra9/aWuvvVd7y7Hcfc4za80927CQhUa6vhcJerTOYysyzzjmWhgwmLVgUo7fApCAxNRGRzFHIZnFUO4OP5Gker1WLr1q0YM2ZIg8IqNd7NMR/JMW/G7C/mEREREUmAoYmIiIjIDAxNRERERGZgaCIiIiIyA0MTERERkRkYmoiIiIjMwNBEREREZAaGJiIiIiIzMDQRERERmYGhiYiIiMgMDE1EREREZmBoIiIiIjIDQxMRERGRGRiaiIiIiMzgIHUBtkgIAQDQaDQWP7ZWq0VZWRk0Gg2USqXFj09147hLg+MuDY679XHMpXH7uBv+3Tb8O25pDE11KC4uBgAEBQVJXAkRERE1VHFxMdzd3S1+XJloqjhmx/R6PbKysuDq6gqZTGbRY2s0GgQFBeHSpUtwc3Oz6LGpfhx3aXDcpcFxtz6OuTRuH3chBIqLixEQEAC53PIzkHilqQ5yuRxt27Zt0vdwc3Pjf1gS4LhLg+MuDY679XHMpXHruDfFFSYDTgQnIiIiMgNDExEREZEZGJqsTK1WY/78+VCr1VKX0qJw3KXBcZcGx936OObSsPa4cyI4ERERkRl4pYmIiIjIDAxNRERERGZgaCIiIiIyA0MTERERkRkYmqxo+fLlCA4OhqOjIwYMGIBDhw5JXZJde/vttyGTyUy+unTpYtxeUVGBmJgYtGnTBi4uLnjssceQm5trcozMzEyMHTsWzs7O8PHxwauvvorq6mprn4pN++233/DQQw8hICAAMpkMGzduNNkuhMC8efPg7+8PJycnREZG4ty5cyZ9CgoKMHHiRLi5ucHDwwN/+9vfUFJSYtLnxIkTGDx4MBwdHREUFIR///vfTX1qNu1u4/7cc8/V+vkfNWqUSR+Oe8MsWrQI/fv3h6urK3x8fDBhwgSkpKSY9LHU75WEhAT07dsXarUaHTt2xOrVq5v69GyWOeM+bNiwWj/vL7zwgkkfq4y7IKv4/vvvhUqlEqtWrRKnTp0SU6ZMER4eHiI3N1fq0uzW/PnzRffu3UV2drbx6+rVq8btL7zwgggKChLx8fEiMTFR/OlPfxIDBw40bq+urhZhYWEiMjJSHD16VGzdulV4eXmJuXPnSnE6Nmvr1q3izTffFD/99JMAIDZs2GCy/f333xfu7u5i48aN4vjx4+Lhhx8WISEhory83Nhn1KhRolevXuLAgQPi999/Fx07dhRPPvmkcXtRUZHw9fUVEydOFMnJyeK7774TTk5O4vPPP7fWadqcu437pEmTxKhRo0x+/gsKCkz6cNwbZuTIkeLLL78UycnJ4tixY2LMmDGiXbt2oqSkxNjHEr9XLly4IJydncXs2bPF6dOnxSeffCIUCoXYvn27Vc/XVpgz7kOHDhVTpkwx+XkvKioybrfWuDM0Wcn9998vYmJijK91Op0ICAgQixYtkrAq+zZ//nzRq1evOrcVFhYKpVIp1q1bZ2w7c+aMACD2798vhKj5R0kul4ucnBxjn88++0y4ubmJysrKJq3dXt3+j7derxd+fn5i8eLFxrbCwkKhVqvFd999J4QQ4vTp0wKAOHz4sLHPtm3bhEwmE1euXBFCCPHpp58KT09Pk3GfM2eO6Ny5cxOfkX2oLzSNHz++3n047vcuLy9PABB79uwRQlju98prr70munfvbvJe0dHRYuTIkU19Snbh9nEXoiY0zZgxo959rDXuvD1nBVVVVUhKSkJkZKSxTS6XIzIyEvv375ewMvt37tw5BAQEoEOHDpg4cSIyMzMBAElJSdBqtSZj3qVLF7Rr18445vv370ePHj3g6+tr7DNy5EhoNBqcOnXKuidip9LT05GTk2Myzu7u7hgwYIDJOHt4eKBfv37GPpGRkZDL5Th48KCxz5AhQ6BSqYx9Ro4ciZSUFFy/ft1KZ2N/EhIS4OPjg86dO2PatGm4du2acRvH/d4VFRUBAFq3bg3Acr9X9u/fb3IMQx/+e1Dj9nE3+Pbbb+Hl5YWwsDDMnTsXZWVlxm3WGnc+sNcK8vPzodPpTP4yAcDX1xdnz56VqCr7N2DAAKxevRqdO3dGdnY2FixYgMGDByM5ORk5OTlQqVTw8PAw2cfX1xc5OTkAgJycnDr/Tgzb6O4M41TXON46zj4+PibbHRwc0Lp1a5M+ISEhtY5h2Obp6dkk9duzUaNG4dFHH0VISAjOnz+PN954A6NHj8b+/fuhUCg47vdIr9dj5syZGDRoEMLCwgDAYr9X6uuj0WhQXl4OJyenpjglu1DXuAPAU089hfbt2yMgIAAnTpzAnDlzkJKSgp9++gmA9cadoYns1ujRo41/7tmzJwYMGID27dvjhx9+aNG/dKhleOKJJ4x/7tGjB3r27InQ0FAkJCRg+PDhElbWPMTExCA5ORl//PGH1KW0KPWN+z/+8Q/jn3v06AF/f38MHz4c58+fR2hoqNXq4+05K/Dy8oJCoaj1CYvc3Fz4+flJVFXz4+HhgU6dOiEtLQ1+fn6oqqpCYWGhSZ9bx9zPz6/OvxPDNro7wzjd6Wfbz88PeXl5Jturq6tRUFDAvwsL6tChA7y8vJCWlgaA434vYmNjsXnzZuzevRtt27Y1tlvq90p9fdzc3Fr0//DVN+51GTBgAACY/LxbY9wZmqxApVIhPDwc8fHxxja9Xo/4+HhERERIWFnzUlJSgvPnz8Pf3x/h4eFQKpUmY56SkoLMzEzjmEdERODkyZMm/7DExcXBzc0N3bp1s3r99igkJAR+fn4m46zRaHDw4EGTcS4sLERSUpKxz65du6DX642/+CIiIvDbb79Bq9Ua+8TFxaFz584t+hZRQ1y+fBnXrl2Dv78/AI57YwghEBsbiw0bNmDXrl21bl1a6vdKRESEyTEMfVrqvwd3G/e6HDt2DABMft6tMu5mTxmne/L9998LtVotVq9eLU6fPi3+8Y9/CA8PD5OZ/tQwL7/8skhISBDp6eli7969IjIyUnh5eYm8vDwhRM1Hg9u1ayd27dolEhMTRUREhIiIiDDub/iIalRUlDh27JjYvn278Pb25pIDtykuLhZHjx4VR48eFQDEkiVLxNGjR8XFixeFEDVLDnh4eIhNmzaJEydOiPHjx9e55ECfPn3EwYMHxR9//CHuu+8+k4++FxYWCl9fX/HMM8+I5ORk8f333wtnZ+cW+9F3Ie487sXFxeKVV14R+/fvF+np6WLnzp2ib9++4r777hMVFRXGY3DcG2batGnC3d1dJCQkmHy0vayszNjHEr9XDB99f/XVV8WZM2fE8uXLW/SSA3cb97S0NPHOO++IxMREkZ6eLjZt2iQ6dOgghgwZYjyGtcadocmKPvnkE9GuXTuhUqnE/fffLw4cOCB1SXYtOjpa+Pv7C5VKJQIDA0V0dLRIS0szbi8vLxfTp08Xnp6ewtnZWTzyyCMiOzvb5BgZGRli9OjRwsnJSXh5eYmXX35ZaLVaa5+KTdu9e7cAUOtr0qRJQoiaZQfeeust4evrK9RqtRg+fLhISUkxOca1a9fEk08+KVxcXISbm5uYPHmyKC4uNulz/Phx8ec//1mo1WoRGBgo3n//fWudok2607iXlZWJqKgo4e3tLZRKpWjfvr2YMmVKrf8J47g3TF3jDUB8+eWXxj6W+r2ye/du0bt3b6FSqUSHDh1M3qOludu4Z2ZmiiFDhojWrVsLtVotOnbsKF599VWTdZqEsM64y24UTERERER3wDlNRERERGZgaCIiIiIyA0MTERERkRkYmoiIiIjMwNBEREREZAaGJiIiIiIzMDQRERERmYGhiYgIQHBwMJYuXSp1GURkwxiaiMjqnnvuOUyYMAEAMGzYMMycOdNq77169Wp4eHjUaj98+LDJk9SJiG7nIHUBRESWUFVVBZVK1ej9vb29LVgNETVHvNJERJJ57rnnsGfPHnz88ceQyWSQyWTIyMgAACQnJ2P06NFwcXGBr68vnnnmGeTn5xv3HTZsGGJjYzFz5kx4eXlh5MiRAIAlS5agR48eaNWqFYKCgjB9+nSUlJQAABISEjB58mQUFRUZ3+/tt98GUPv2XGZmJsaPHw8XFxe4ubnh8ccfR25urnH722+/jd69e+Prr79GcHAw3N3d8cQTT6C4uNjYZ/369ejRowecnJzQpk0bREZGorS0tIlGk4iaGkMTEUnm448/RkREBKZMmYLs7GxkZ2cjKCgIhYWFePDBB9GnTx8kJiZi+/btyM3NxeOPP26y/1dffQWVSoW9e/dixYoVAAC5XI7//ve/OHXqFL766ivs2rULr732GgBg4MCBWLp0Kdzc3Izv98orr9SqS6/XY/z48SgoKMCePXsQFxeHCxcuIDo62qTf+fPnsXHjRmzevBmbN2/Gnj178P777wMAsrOz8eSTT+L555/HmTNnkJCQgEcffRR83CeR/eLtOSKSjLu7O1QqFZydneHn52dsX7ZsGfr06YOFCxca21atWoWgoCCkpqaiU6dOAID77rsP//73v02Oeev8qODgYLz77rt44YUX8Omnn0KlUsHd3R0ymczk/W4XHx+PkydPIj09HUFBQQCA//3vf+jevTsOHz6M/v37A6gJV6tXr4arqysA4JlnnkF8fDzee+89ZGdno7q6Go8++ijat28PAOjRo8c9jBYRSY1XmojI5hw/fhy7d++Gi4uL8atLly4Aaq7uGISHh9fad+fOnRg+fDgCAwPh6uqKZ555BteuXUNZWZnZ73/mzBkEBQUZAxMAdOvWDR4eHjhz5oyxLTg42BiYAMDf3x95eXkAgF69emH48OHo0aMH/vrXv2LlypW4fv26+YNARDaHoYmIbE5JSQkeeughHDt2zOTr3LlzGDJkiLFfq1atTPbLyMjAuHHj0LNnT/z4449ISkrC8uXLAdRMFLc0pVJp8lomk0Gv1wMAFAoF4uLisG3bNnTr1g2ffPIJOnfujPT0dIvXQUTWwdBERJJSqVTQ6XQmbX379sWpU6cQHByMjh07mnzdHpRulZSUBL1ej48++gh/+tOf0KlTJ2RlZd31/W7XtWtXXLp0CZcuXTK2nT59GoWFhejWrZvZ5yaTyTBo0CAsWLAAR48ehUqlwoYNG8zen4hsC0MTEUkqODgYBw8eREZGBvLz86HX6xETE4OCggI8+eSTOHz4MM6fP48dO3Zg8uTJdww8HTt2hFarxSeffIILFy7g66+/Nk4Qv/X9SkpKEB8fj/z8/Dpv20VGRqJHjx6YOHEijhw5gkOHDuHZZ5/F0KFD0a9fP7PO6+DBg1i4cCESExORmZmJn376CVevXkXXrl0bNkBEZDMYmohIUq+88goUCgW6desGb29vZGZmIiAgAHv37oVOp0NUVBR69OiBmTNnwsPDA3J5/b+2evXqhSVLluCDDz5AWFgYvv32WyxatMikz8CBA/HCCy8gOjoa3t7etSaSAzVXiDZt2gRPT08MGTIEkZGR6NChA9auXWv2ebm5ueG3337DmDFj0KlTJ/zzn//ERx99hNGjR5s/OERkU2SCn38lIiIiuiteaSIiIiIyA0MTERERkRkYmoiIiIjMwNBEREREZAaGJiIiIiIzMDQRERERmYGhiYiIiMgMDE1EREREZmBoIiIiIjIDQxMRERGRGRiaiIiIiMzA0ERERERkhv8PjI8ckZxGNt4AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from eval_utils import *\n",
    "from visualization import *\n",
    "\n",
    "print_report(parameters)\n",
    "plot_costs(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea6b040d-9c52-4c59-9dba-d2846a411e9b",
   "metadata": {},
   "source": [
    "# Let's try with Learning decays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3189d4b8-cfe8-40dd-a8a0-b81e12703c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(i):\n",
    "    if i < 500:\n",
    "        return 0.008\n",
    "    elif i < 1000:\n",
    "        return 0.001\n",
    "    elif i < 1500:\n",
    "        return 0.0001\n",
    "    elif i < 2000:\n",
    "        return 0.00001\n",
    "    else:\n",
    "        return 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "733c586a-4d23-4762-aea6-6828a865a448",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L_layer_model_with_leaning_decay(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False, X_test=None, Y_test=None):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    # np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "        \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        ##update learnign rate digital\n",
    "        learning_rate = lr_schedule(i)\n",
    "        \n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "                \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL,Y)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        # Update parameters.\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                        \n",
    "        # Print the cost every 100 iterations and for the last iteration\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    if X_test is not None and Y_test is not None:\n",
    "        train_pred = predict(train_x,parameters)\n",
    "        test_pred = predict(test_x, parameters)\n",
    "        train_acc = accuracy(train_pred, train_y)\n",
    "        test_acc = accuracy(test_pred, test_y)\n",
    "        print(f\"Final train accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"Final test accuracy:  {test_acc:.2f}%\")\n",
    "\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4763d6fb-6b76-47f7-a3e5-3318a4bb874c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693165156540514\n",
      "Cost after iteration 100: 0.6623214547569527\n",
      "Cost after iteration 200: 0.6418261826320482\n",
      "Cost after iteration 300: 0.6285308544037146\n",
      "Cost after iteration 400: 0.619900474409694\n",
      "Cost after iteration 500: 0.6143189577668501\n",
      "Cost after iteration 600: 0.6137796682761515\n",
      "Cost after iteration 700: 0.6132695998052063\n",
      "Cost after iteration 800: 0.612787234898859\n",
      "Cost after iteration 900: 0.6123311298703649\n",
      "Cost after iteration 1000: 0.6118999089107415\n",
      "Cost after iteration 1100: 0.6118581105692612\n",
      "Cost after iteration 1200: 0.6118165468788133\n",
      "Cost after iteration 1300: 0.6117752166221243\n",
      "Cost after iteration 1400: 0.6117341183396212\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m layers_dims = [\u001b[32m12288\u001b[39m, \u001b[32m64\u001b[39m, \u001b[32m16\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m1\u001b[39m] \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m parameters, costs = \u001b[43mL_layer_model_with_leaning_decay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m0.008\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43mnum_iterations\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m2500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprint_cost\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m=\u001b[49m\u001b[43mY_test_org\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 37\u001b[39m, in \u001b[36mL_layer_model_with_leaning_decay\u001b[39m\u001b[34m(X, Y, layers_dims, learning_rate, num_iterations, print_cost, X_test, Y_test)\u001b[39m\n\u001b[32m     34\u001b[39m cost = compute_cost(AL,Y)\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Backward propagation.\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m grads = \u001b[43mL_model_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mAL\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaches\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Update parameters.\u001b[39;00m\n\u001b[32m     41\u001b[39m parameters = update_parameters(parameters, grads, learning_rate)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/My_projects /Damage_Vision_project/notebooks/../src/L_layer_model_03/L_layer_model.py:332\u001b[39m, in \u001b[36mL_model_backward\u001b[39m\u001b[34m(AL, Y, caches)\u001b[39m\n\u001b[32m    329\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mrange\u001b[39m(L-\u001b[32m1\u001b[39m)):\n\u001b[32m    330\u001b[39m     \u001b[38;5;66;03m# lth layer: (RELU -> LINEAR) gradients.\u001b[39;00m\n\u001b[32m    331\u001b[39m     current_cache = caches[l]\n\u001b[32m--> \u001b[39m\u001b[32m332\u001b[39m     dA_prev_temp, dW_temp, db_temp = \u001b[43mlinear_activation_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgrads\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdA\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcurrent_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrelu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    333\u001b[39m     grads[\u001b[33m\"\u001b[39m\u001b[33mdA\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(l)] = dA_prev_temp\n\u001b[32m    334\u001b[39m     grads[\u001b[33m\"\u001b[39m\u001b[33mdW\u001b[39m\u001b[33m\"\u001b[39m + \u001b[38;5;28mstr\u001b[39m(l + \u001b[32m1\u001b[39m)] = dW_temp\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/My_projects /Damage_Vision_project/notebooks/../src/L_layer_model_03/L_layer_model.py:289\u001b[39m, in \u001b[36mlinear_activation_backward\u001b[39m\u001b[34m(dA, cache, activation)\u001b[39m\n\u001b[32m    287\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m activation == \u001b[33m\"\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    288\u001b[39m     dZ = relu_backward(dA, activation_cache)\n\u001b[32m--> \u001b[39m\u001b[32m289\u001b[39m     dA_prev, dW, db = \u001b[43mlinear_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlinear_cache\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m activation == \u001b[33m\"\u001b[39m\u001b[33msigmoid\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    292\u001b[39m     dZ = sigmoid_backward(dA, activation_cache)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/My_projects /Damage_Vision_project/notebooks/../src/L_layer_model_03/L_layer_model.py:263\u001b[39m, in \u001b[36mlinear_backward\u001b[39m\u001b[34m(dZ, cache)\u001b[39m\n\u001b[32m    261\u001b[39m dW = \u001b[32m1.\u001b[39m/m * np.dot(dZ,A_prev.T)\n\u001b[32m    262\u001b[39m db = \u001b[32m1.\u001b[39m/m * np.sum(dZ, axis = \u001b[32m1\u001b[39m, keepdims = \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m--> \u001b[39m\u001b[32m263\u001b[39m dA_prev = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mW\u001b[49m\u001b[43m.\u001b[49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdZ\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    265\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (dA_prev.shape == A_prev.shape)\n\u001b[32m    266\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m (dW.shape == W.shape)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "layers_dims = [12288, 64, 16, 5, 1] \n",
    "\n",
    "parameters, costs = L_layer_model_with_leaning_decay(train_x, train_y, layers_dims, learning_rate = 0.008,num_iterations = 2500, print_cost = True, X_test=test_x, Y_test=Y_test_org)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03db9d79-1f83-48e1-874a-19d4ad79efc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_utils import *\n",
    "from visualization import *\n",
    "\n",
    "print_report(parameters)\n",
    "plot_costs(costs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2956f3df-e60f-4f84-81ba-c948991aa817",
   "metadata": {},
   "source": [
    "# **It didn't do much so i think the problem is with the depth of the NN is not enough to recognize damaged buildings**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d0e1995-75f0-4655-bb0f-3962cb3d4a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L_layer_model_test(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False, X_test=None, Y_test=None):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    # np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "        \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "                \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL,Y)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        # Update parameters.\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                        \n",
    "        # Print the cost every 100 iterations and for the last iteration\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    if X_test is not None and Y_test is not None:\n",
    "        train_pred = predict(train_x,parameters)\n",
    "        test_pred = predict(test_x, parameters)\n",
    "        train_acc = accuracy(train_pred, train_y)\n",
    "        test_acc = accuracy(test_pred, test_y)\n",
    "        print(f\"Final train accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"Final test accuracy:  {test_acc:.2f}%\")\n",
    "\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d54f3c20-e57e-4104-900e-be7d738ece83",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 25  # or 10\n",
    "\n",
    "train_x_tiny = train_x[:, :k]\n",
    "train_y_tiny = train_y[:, :k]\n",
    "test_y_tiny = test_y[:, :k]\n",
    "test_x_tiny= test_x[:, :k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0c4908d-435a-4820-a5e7-294a02973108",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6930852272806844\n",
      "Cost after iteration 100: 0.6594483457482012\n",
      "Cost after iteration 200: 0.6280974712157471\n",
      "Cost after iteration 300: 0.5983759089356763\n",
      "Cost after iteration 400: 0.5705890618189693\n",
      "Cost after iteration 500: 0.5446223262603536\n",
      "Cost after iteration 600: 0.520276113934339\n",
      "Cost after iteration 700: 0.49741706054751\n",
      "Cost after iteration 800: 0.47592821038875555\n",
      "Cost after iteration 900: 0.45570574362946226\n",
      "Cost after iteration 1000: 0.43665911471191216\n",
      "Cost after iteration 1100: 0.4187149079078097\n",
      "Cost after iteration 1200: 0.4018086077995168\n",
      "Cost after iteration 1300: 0.3858838764037803\n",
      "Cost after iteration 1400: 0.3708908293203533\n",
      "Cost after iteration 1500: 0.35678457716325623\n",
      "Cost after iteration 1600: 0.34352388475197876\n",
      "Cost after iteration 1700: 0.3310701198203266\n",
      "Cost after iteration 1800: 0.31938674146748847\n",
      "Cost after iteration 1900: 0.3084385600643721\n",
      "Cost after iteration 2000: 0.2981914869367704\n",
      "Cost after iteration 2100: 0.2886121959840878\n",
      "Cost after iteration 2200: 0.27966807735801025\n",
      "Cost after iteration 2300: 0.2713270466833307\n",
      "Cost after iteration 2400: 0.2635575813883994\n",
      "Cost after iteration 2499: 0.2563984429318268\n",
      "Final train accuracy: 70.71%\n",
      "Final test accuracy:  74.01%\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [12288, 64, 16, 5, 1] \n",
    "parameters, costs = L_layer_model_test(train_x_tiny, train_y_tiny, layers_dims, learning_rate = 0.001,num_iterations = 2500, print_cost = True, X_test=test_y_tiny, Y_test=test_x_tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "83ad4808-2cc9-4ffe-923a-296dbad73c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, learning_rate, num_iterations = 15000, print_cost = True, X_test=None, Y_test=None):\n",
    "    \"\"\"\n",
    "    Implements a three-layer neural network: LINEAR->RELU->LINEAR->RELU->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (containing 0 for red dots; 1 for blue dots), of shape (1, number of examples)\n",
    "    learning_rate -- learning rate for gradient descent \n",
    "    num_iterations -- number of iterations to run gradient descent\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    initialization -- flag to choose which initialization to use (\"zeros\",\"random\" or \"he\")\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model\n",
    "    \"\"\"\n",
    "        \n",
    "    grads = {}\n",
    "    costs = [] # to keep track of the loss\n",
    "    m = X.shape[1] # number of examples\n",
    "    \n",
    "    # Initialize parameters dictionary.\n",
    "    parameters = initialize_parameters_he(layers_dims)\n",
    "\n",
    "    # Loop (gradient descent)\n",
    "\n",
    "    for i in range(num_iterations):\n",
    "\n",
    "        # Forward propagation: LINEAR -> RELU -> LINEAR -> RELU -> LINEAR -> SIGMOID.\n",
    "        a3, cache = forward_propagation(X, parameters)\n",
    "        \n",
    "        # Loss\n",
    "        cost = compute_loss(a3, Y)\n",
    "\n",
    "        # Backward propagation.\n",
    "        grads = backward_propagation(X, Y, cache)\n",
    "        \n",
    "        # Update parameters.\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "        \n",
    "        # Print the loss every 1000 iterations\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print(\"Cost after iteration {}: {}\".format(i, cost))\n",
    "            costs.append(cost)\n",
    "    if X_test is not None and Y_test is not None:\n",
    "        train_pred = predict(train_x,train_y,parameters)\n",
    "        test_pred = predict(test_x,test_y, parameters)\n",
    "        print(f\"Final train accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"Final test accuracy:  {test_acc:.2f}%\")\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92d8f357-82d0-44be-8291-4b49a9426b54",
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 25  # or 10\n",
    "\n",
    "train_x_tiny = train_x[:, :k]\n",
    "train_y_tiny = train_y[:, :k]\n",
    "test_y_tiny = test_y[:, :k]\n",
    "test_x_tiny= test_x[:, :k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "afc18f5f-ea69-440b-a643-e0ebebbd55dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6931472395100389\n",
      "Cost after iteration 100: 0.5261529635681322\n",
      "Cost after iteration 200: 0.42282997424912794\n",
      "Cost after iteration 300: 0.35635196534483504\n",
      "Cost after iteration 400: 0.31169336242918616\n",
      "Cost after iteration 500: 0.2804779102948433\n",
      "Cost after iteration 600: 0.25789853284031594\n",
      "Cost after iteration 700: 0.24108644270708834\n",
      "Cost after iteration 800: 0.22826019015470156\n",
      "Cost after iteration 900: 0.21827190823998496\n",
      "Cost after iteration 1000: 0.21035704041691983\n",
      "Cost after iteration 1100: 0.20399116435515247\n",
      "Cost after iteration 1200: 0.19880513494074545\n",
      "Cost after iteration 1300: 0.19453310499015003\n",
      "Cost after iteration 1400: 0.1909797132716559\n",
      "Cost after iteration 1500: 0.1879987997406377\n",
      "Cost after iteration 1600: 0.1854792537345515\n",
      "Cost after iteration 1700: 0.183335392721005\n",
      "Cost after iteration 1800: 0.1815002893424313\n",
      "Cost after iteration 1900: 0.17992105686761123\n",
      "Cost after iteration 2000: 0.178555463037299\n",
      "Cost after iteration 2100: 0.17736946067961346\n",
      "Cost after iteration 2200: 0.17633536202027472\n",
      "Cost after iteration 2300: 0.17543047136946235\n",
      "Cost after iteration 2400: 0.17463604906484778\n",
      "Cost after iteration 2499: 0.17394308831984556\n",
      "Final train accuracy: 0.71%\n",
      "Final test accuracy:  0.74%\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [3072, 256, 64, 16, 1] \n",
    "parameters, costs = L_layer_model(train_x_tiny, train_y_tiny, layers_dims, learning_rate = 0.01,num_iterations = 2500, print_cost = True, X_test=test_y_tiny, Y_test=test_x_tiny)\n",
    "# parameters, costs = model(train_x_tiny, train_y_tiny, layers_dims, learning_rate = 0.001,num_iterations = 3500, print_cost = True, X_test=test_y_tiny, Y_test=test_x_tiny)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1f4917fb-0dc9-4174-b3bc-be9a9c4216c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_schedule(i):\n",
    "    if i < 500:\n",
    "        return 0.008\n",
    "    elif i < 1000:\n",
    "        return 0.001\n",
    "    elif i < 1500:\n",
    "        return 0.0001\n",
    "    elif i < 2000:\n",
    "        return 0.00001\n",
    "    else:\n",
    "        return 0.01\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "31312126-9249-42d0-94d5-7298922c612d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L_layer_model_with_leaning_decay(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False, X_test=None, Y_test=None):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    # np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "        \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        ##update learnign rate digital\n",
    "        learning_rate = lr_schedule(i)\n",
    "        \n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "                \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL,Y)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        # Update parameters.\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                        \n",
    "        # Print the cost every 100 iterations and for the last iteration\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    if X_test is not None and Y_test is not None:\n",
    "        train_pred = predict(X, parameters)\n",
    "        test_pred  = predict(X_test, parameters)\n",
    "    \n",
    "        train_acc = accuracy(train_pred, Y)\n",
    "        test_acc  = accuracy(test_pred, Y_test)\n",
    "    \n",
    "        print(f\"Final train accuracy: {train_acc*100:.2f}%\")\n",
    "        print(f\"Final test accuracy:  {test_acc*100:.2f}%\")\n",
    "\n",
    "\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d12f36b2-33a1-4275-ac23-a8cc82717fbc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.6931470888943896\n",
      "Cost after iteration 100: 0.553257674953872\n",
      "Cost after iteration 200: 0.45845539440361516\n",
      "Cost after iteration 300: 0.39269821567945784\n",
      "Cost after iteration 400: 0.345807245437106\n",
      "Cost after iteration 500: 0.3114425066815451\n",
      "Cost after iteration 600: 0.3078130077556848\n",
      "Cost after iteration 700: 0.30430913111699975\n",
      "Cost after iteration 800: 0.3009253150627276\n",
      "Cost after iteration 900: 0.2976562917362491\n",
      "Cost after iteration 1000: 0.294497069612258\n",
      "Cost after iteration 1100: 0.29418702567586424\n",
      "Cost after iteration 1200: 0.29387802745776836\n",
      "Cost after iteration 1300: 0.29357007039022104\n",
      "Cost after iteration 1400: 0.2932631499295524\n",
      "Cost after iteration 1500: 0.29295726155606483\n",
      "Cost after iteration 1600: 0.2929267295413398\n",
      "Cost after iteration 1700: 0.29289620779784864\n",
      "Cost after iteration 1800: 0.2928656963211064\n",
      "Cost after iteration 1900: 0.2928351951066415\n",
      "Cost after iteration 2000: 0.29280470414997695\n",
      "Cost after iteration 2100: 0.266770245650488\n",
      "Cost after iteration 2200: 0.24761577044360983\n",
      "Cost after iteration 2300: 0.2331483696465552\n",
      "Cost after iteration 2400: 0.22197742673930448\n",
      "Cost after iteration 2499: 0.21326801521848893\n",
      "Final train accuracy: 96.00%\n",
      "Final test accuracy:  96.00%\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [3072, 256, 64, 16, 1] \n",
    "parameters, costs = L_layer_model_with_leaning_decay(\n",
    "    train_x_tiny,\n",
    "    train_y_tiny,\n",
    "    layers_dims,\n",
    "    learning_rate=0.01,\n",
    "    num_iterations=2500,\n",
    "    print_cost=True,\n",
    "    X_test=test_x_tiny,\n",
    "    Y_test=test_y_tiny\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2b057208-69d3-432d-8a18-e3059d8f0470",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_x_tiny: (3072, 25)\n",
      "train_y_tiny: (1, 25)\n",
      "test_x_tiny: (3072, 25)\n",
      "test_y_tiny: (1, 25)\n",
      "Unique predictions: [1.]\n"
     ]
    }
   ],
   "source": [
    "print(\"train_x_tiny:\", train_x_tiny.shape)\n",
    "print(\"train_y_tiny:\", train_y_tiny.shape)\n",
    "print(\"test_x_tiny:\", test_x_tiny.shape)\n",
    "print(\"test_y_tiny:\", test_y_tiny.shape)\n",
    "\n",
    "preds = predict(train_x_tiny, parameters)\n",
    "print(\"Unique predictions:\", np.unique(preds))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1909052a-c119-4376-adfd-0b6eb9315fa1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train positives: 1847\n",
      "Train negatives: 765\n",
      "Test positives: 242\n",
      "Test negatives: 85\n"
     ]
    }
   ],
   "source": [
    "print(\"Train positives:\", np.sum(train_y))\n",
    "print(\"Train negatives:\", train_y.shape[1] - np.sum(train_y))\n",
    "\n",
    "print(\"Test positives:\", np.sum(test_y))\n",
    "print(\"Test negatives:\", test_y.shape[1] - np.sum(test_y))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d02040-8934-433e-870c-ac1217006ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

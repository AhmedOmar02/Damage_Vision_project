{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b3844f-830f-43b3-94bb-170667d966f5",
   "metadata": {},
   "source": [
    "## Optimized Model: Rethinking the Pipeline\n",
    "\n",
    "In the previous notebooks, I implemented multiple models of increasing complexity, ranging from logistic regression to deeper neural networks. While these experiments were valuable for understanding core concepts in machine learning and deep learning, the resulting performance remained relatively low and inconsistent.\n",
    "\n",
    "After analyzing the results more carefully, it became clear that the limitation was not only related to model architecture or optimization techniques, but also to **how the data was labeled and interpreted**.\n",
    "\n",
    "### Identified Issue with Data Labeling\n",
    "\n",
    "In the earlier approach, an image was classified as *damaged* if **any single polygon** within the image was labeled as `D_Building` or `Debris`. This means that even a small, localized damaged region could cause the entire image to be labeled as damaged.  \n",
    "Such a strategy likely introduces noise and label ambiguity, especially for images that are largely intact but contain minor damage.\n",
    "\n",
    "This coarse labeling scheme may prevent the model from learning meaningful visual patterns related to *overall structural damage*, which is the core objective of this project.\n",
    "\n",
    "### Objective of This Notebook\n",
    "\n",
    "In this notebook, I aim to build the **most optimized model so far**, not only by:\n",
    "- improving model architecture,\n",
    "- applying better initialization, regularization, and optimization techniques,\n",
    "\n",
    "but also by **revisiting and refining the data labeling strategy itself**.\n",
    "\n",
    "By aligning the labels more closely with the true semantic meaning of structural damage, the goal is to provide the model with cleaner supervision and enable more reliable learning.\n",
    "\n",
    "This step marks a transition from experimenting with models to **systematically improving the full machine learning pipeline**, from data understanding to final evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e76b5-52c2-4c6a-b0cb-e65bad887e86",
   "metadata": {},
   "source": [
    "## Parsing the XML file & Preparing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bd2b4b44-6618-478f-b0a2-0472aa6d6665",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py \n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from Optimized_Model_04.Optimized_Model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f475d2f-3c28-4279-8781-cbeffd231d07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 2612\n",
      "Number of testing examples: m_test = 327\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (2612, 64, 64, 3)\n",
      "train_set_y shape: (1, 2612)\n",
      "test_set_x shape: (327, 64, 64, 3)\n",
      "test_set_y shape: (1, 327)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"../EIDSeg_Dataset/cache/eidseg_64x64_binary_any.h5\", \"r\") as f:\n",
    "    X_train_org = f[\"X_train\"][:]\n",
    "    Y_train_org = f[\"Y_train\"][:]\n",
    "    X_test_org  = f[\"X_test\"][:]\n",
    "    Y_test_org  = f[\"Y_test\"][:]\n",
    "\n",
    "m_train = X_train_org.shape[0]\n",
    "m_test = X_test_org.shape[0]\n",
    "num_px =X_train_org.shape[1]\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(X_train_org.shape))\n",
    "print (\"train_set_y shape: \" + str(Y_train_org.shape))\n",
    "print (\"test_set_x shape: \" + str(X_test_org.shape))\n",
    "print (\"test_set_y shape: \" + str(Y_test_org.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c867baa-be67-47fc-b762-59fcf6120a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x flatten shape: (12288, 2612)\n",
      "train_set_y shape: (1, 2612)\n",
      "test_set_x flatten shape: (12288, 327)\n",
      "test_set_y shape: (1, 327)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"../EIDSeg_Dataset/cache//eidseg_64x64_binary_any_flat.h5\", \"r\") as f:\n",
    "    train_x = f[\"train_x\"][:]   # (12288, m)\n",
    "    train_y = f[\"train_y\"][:]   # (1, m)\n",
    "    test_x  = f[\"test_x\"][:]    # (12288, m)\n",
    "    test_y  = f[\"test_y\"][:]    # (1, m)\n",
    "\n",
    "print (\"train_set_x flatten shape: \" + str(train_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_y.shape))\n",
    "print (\"test_set_x flatten shape: \" + str(test_x.shape))\n",
    "print (\"test_set_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17cb8a94-f644-4ed6-8624-6948a292ba87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/400 - cost: 0.671243\n",
      "Epoch 40/400 - cost: 0.604665\n",
      "Epoch 80/400 - cost: 0.604453\n",
      "Epoch 120/400 - cost: 0.604235\n",
      "Epoch 160/400 - cost: 0.604018\n",
      "Epoch 200/400 - cost: 0.603751\n",
      "Epoch 240/400 - cost: 0.603354\n",
      "Epoch 280/400 - cost: 0.602917\n",
      "Epoch 320/400 - cost: 0.602342\n",
      "Epoch 360/400 - cost: 0.601625\n",
      "Train accuracy: 70.71209800918837\n",
      "Test accuracy: 74.00611620795107\n"
     ]
    }
   ],
   "source": [
    "#layers_dims = [12288, 64, 32, 16, 1] \n",
    "layers_dims = [12288, 128, 64, 32, 1]  # try modest sizes for FC net\n",
    "params, costs = L_layer_model_mini_batch(train_x, train_y, layers_dims,\n",
    "                              learning_rate=0.01,\n",
    "                              num_epochs=400,\n",
    "                              mini_batch_size=64,\n",
    "                              print_cost=True,\n",
    "                              seed=1,\n",
    "                              init=\"he\")\n",
    "\n",
    "# evaluate\n",
    "preds_train = predict(train_x, params)\n",
    "acc_train = np.mean(preds_train == train_y) * 100\n",
    "print(\"Train accuracy:\", acc_train)\n",
    "\n",
    "preds_test = predict(test_x, params)\n",
    "acc_test = np.mean(preds_test == test_y) * 100\n",
    "print(\"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd37bada-5bd9-4108-9eba-e24fa81bd317",
   "metadata": {},
   "source": [
    "# Final Notes (I declare defeat )\n",
    "## After two weeks of trying, I give myself a pat on the back\n",
    "## I initially thought I could build a working model without convolutional networks. After trying many approaches changing the data, adjusting the model, and testing different hyperparameters I realized that this task is best suited for CNNs.\n",
    "\n",
    "## because of the dataset imbalance and the early 70% accuracy (cause the model was learing to put all ones), I got false hope. but it was a valuable learning experience. I learned my lesson and now understand the importance of using the right architecture for the problem X0. \n",
    "## even though i hated my life but it was a funny experince to see how many times i thought i found the sultion\n",
    "# Thank you for reading my notebook :)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

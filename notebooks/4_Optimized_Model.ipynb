{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b3844f-830f-43b3-94bb-170667d966f5",
   "metadata": {},
   "source": [
    "## Optimized Model: Rethinking the Pipeline\n",
    "\n",
    "In the previous notebooks, I implemented multiple models of increasing complexity, ranging from logistic regression to deeper neural networks. While these experiments were valuable for understanding core concepts in machine learning and deep learning, the resulting performance remained relatively low and inconsistent.\n",
    "\n",
    "After analyzing the results more carefully, it became clear that the limitation was not only related to model architecture or optimization techniques, but also to **how the data was labeled and interpreted**.\n",
    "\n",
    "### Identified Issue with Data Labeling\n",
    "\n",
    "In the earlier approach, an image was classified as *damaged* if **any single polygon** within the image was labeled as `D_Building` or `Debris`. This means that even a small, localized damaged region could cause the entire image to be labeled as damaged.  \n",
    "Such a strategy likely introduces noise and label ambiguity, especially for images that are largely intact but contain minor damage.\n",
    "\n",
    "This coarse labeling scheme may prevent the model from learning meaningful visual patterns related to *overall structural damage*, which is the core objective of this project.\n",
    "\n",
    "### Objective of This Notebook\n",
    "\n",
    "In this notebook, I aim to build the **most optimized model so far**, not only by:\n",
    "- improving model architecture,\n",
    "- applying better initialization, regularization, and optimization techniques,\n",
    "\n",
    "but also by **revisiting and refining the data labeling strategy itself**.\n",
    "\n",
    "By aligning the labels more closely with the true semantic meaning of structural damage, the goal is to provide the model with cleaner supervision and enable more reliable learning.\n",
    "\n",
    "This step marks a transition from experimenting with models to **systematically improving the full machine learning pipeline**, from data understanding to final evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e76b5-52c2-4c6a-b0cb-e65bad887e86",
   "metadata": {},
   "source": [
    "## Parsing the XML file & Preparing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1bae27b1-943d-4ae1-9265-cfbed5fd3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Optimized_Model import *\n",
    "from mini_batch_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c867baa-be67-47fc-b762-59fcf6120a30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final X shape: (2612, 64, 64, 3)\n",
      "Final X shape: (327, 64, 64, 3)\n",
      "Train positive ratio: 0.49119448698315465 n_train: 2612\n",
      "Test  positive ratio: 0.5168195718654435 n_test: 327\n"
     ]
    }
   ],
   "source": [
    "labels_train = parse_destroyed_with_size_check(\"../EIDSeg_Dataset/data/train/train.xml\", min_coverage=0.3)\n",
    "labels_test  = parse_destroyed_with_size_check(\"../EIDSeg_Dataset/data/test/test.xml\",  min_coverage=0.3)\n",
    "\n",
    "X_train_org, ordered_filenames_train = load_and_resize_images(\"../EIDSeg_Dataset/data/train/images/default\", target_size=(64,64))\n",
    "X_test_org,  ordered_filenames_test  = load_and_resize_images(\"../EIDSeg_Dataset/data/test/images/default\",  target_size=(64,64))\n",
    "\n",
    "Y_train_org = build_label_array(ordered_filenames_train, labels_train)   # shape (n_train,)\n",
    "Y_test_org  = build_label_array(ordered_filenames_test,  labels_test)    # shape (n_test,)\n",
    "\n",
    "# quick sanity checks\n",
    "print(\"Train positive ratio:\", Y_train_org.mean(), \"n_train:\", Y_train_org.shape[0])\n",
    "print(\"Test  positive ratio:\", Y_test_org.mean(),  \"n_test:\",  Y_test_org.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d128233-0e35-43ce-bc87-9c19bc96fc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 2612) (1, 2612)\n",
      "(12288, 327) (1, 327)\n"
     ]
    }
   ],
   "source": [
    "train_x = X_train_org.reshape(X_train_org.shape[0], -1).T\n",
    "train_y = Y_train_org.reshape(1,-1)\n",
    "test_x = X_test_org.reshape(X_test_org.shape[0], -1).T\n",
    "test_y = Y_test_org.reshape(1,-1)\n",
    "\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17cb8a94-f644-4ed6-8624-6948a292ba87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/400 - cost: 0.695213\n",
      "Epoch 40/400 - cost: 0.480214\n",
      "Epoch 80/400 - cost: 0.302350\n",
      "Epoch 120/400 - cost: 0.349640\n",
      "Epoch 160/400 - cost: 0.360660\n",
      "Epoch 200/400 - cost: 0.022759\n",
      "Epoch 240/400 - cost: 0.353493\n",
      "Epoch 280/400 - cost: 0.564278\n",
      "Epoch 320/400 - cost: 0.167663\n",
      "Epoch 360/400 - cost: 0.005034\n",
      "Train accuracy: 99.96171516079633\n",
      "Test accuracy: 56.574923547400616\n"
     ]
    }
   ],
   "source": [
    "#layers_dims = [12288, 64, 32, 16, 1] \n",
    "layers_dims = [12288, 128, 64, 32, 1]  # try modest sizes for FC net\n",
    "params, costs = L_layer_model_mini_batch(train_x, train_y, layers_dims,\n",
    "                              learning_rate=0.01,\n",
    "                              num_epochs=400,\n",
    "                              mini_batch_size=64,\n",
    "                              print_cost=True,\n",
    "                              seed=1,\n",
    "                              init=\"he\")\n",
    "\n",
    "# evaluate\n",
    "preds_train = predict(train_x, params)\n",
    "acc_train = np.mean(preds_train == train_y) * 100\n",
    "print(\"Train accuracy:\", acc_train)\n",
    "\n",
    "preds_test = predict(test_x, params)\n",
    "acc_test = np.mean(preds_test == test_y) * 100\n",
    "print(\"Test accuracy:\", acc_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059cb6e9-f824-43b2-8ed3-b4483d7e746c",
   "metadata": {},
   "source": [
    "## L-layer Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e746df3-1539-452d-b594-1c834d839d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "        \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "                \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL,Y)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        # Update parameters.\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                        \n",
    "        # Print the cost every 100 iterations and for the last iteration\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809389d-254c-4202-a050-ed94ccb1512d",
   "metadata": {},
   "source": [
    "## Training!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0439af21-1294-445d-8e22-50a53f36c9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.7246250162012067\n",
      "Cost after iteration 100: 0.6872203231761475\n",
      "Cost after iteration 200: 0.6775461381963849\n",
      "Cost after iteration 300: 0.6660236810493122\n",
      "Cost after iteration 400: 0.675322633125315\n",
      "Cost after iteration 500: 0.6539324891845053\n",
      "Cost after iteration 600: 0.6572655245114436\n",
      "Cost after iteration 700: 0.6285363185367053\n",
      "Cost after iteration 800: 0.6369991025692343\n",
      "Cost after iteration 900: 0.6473037728599877\n",
      "Cost after iteration 1000: 0.5995318317603917\n",
      "Cost after iteration 1100: 0.61476440088501\n",
      "Cost after iteration 1200: 0.6015165843621063\n",
      "Cost after iteration 1300: 0.5705049178075906\n",
      "Cost after iteration 1400: 0.5649023718327169\n",
      "Cost after iteration 1500: 0.5608144433536896\n",
      "Cost after iteration 1600: 0.5818106456401206\n",
      "Cost after iteration 1700: 0.6282697297242217\n",
      "Cost after iteration 1800: 0.5505934307406615\n",
      "Cost after iteration 1900: 0.5623603370943553\n",
      "Cost after iteration 2000: 0.5638067592656174\n",
      "Cost after iteration 2100: 0.5239095162844934\n",
      "Cost after iteration 2200: 0.699562275389274\n",
      "Cost after iteration 2300: 0.4850974434355157\n",
      "Cost after iteration 2400: 0.5930385315473986\n",
      "Cost after iteration 2499: 0.573857897708467\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1] \n",
    "parameters, costs = L_layer_model(train_x, train_y, layers_dims,learning_rate = 0.009, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05d8a8fb-fdd5-4aac-b7e5-4258d1e6a4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Accuracy: 0.8016845329249619\n",
      "Test :Accuracy: 0.5382262996941896\n"
     ]
    }
   ],
   "source": [
    "print(\"Train\", end= \" \")\n",
    "pred_train = predict(train_x, train_y, parameters)\n",
    "print(\"Test\",end= \" \")\n",
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6d16c-9429-41ea-890b-fddaf96a9436",
   "metadata": {},
   "source": [
    "## Let's try Mini-Batch with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0e8f0abe-111f-41ec-9975-1ca8ba52ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = True):\n",
    "    \"\"\"\n",
    "    3-layer neural network model which can be run in different optimizer modes.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    optimizer -- the optimizer to be passed, gradient descent, momentum or adam\n",
    "    layers_dims -- python list, containing the size of each layer\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta -- Momentum hyperparameter\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs\n",
    "    print_cost -- True to print the cost every 1000 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    m = X.shape[1]                   # number of training examples\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        cost_total = 0\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            AL, caches = L_model_forward(minibatch_X, parameters)\n",
    "\n",
    "            # Compute cost and add to the cost total\n",
    "            cost_total += compute_cost(AL, minibatch_Y)* minibatch_Y.shape[1]\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = L_model_backward(AL, minibatch_Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            if optimizer == \"gd\":\n",
    "                #parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "                pass\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        cost_avg = cost_total / m\n",
    "        \n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 10 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d466057-57d7-46be-8e24-bcfc00af8eee",
   "metadata": {},
   "source": [
    "### lets train !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4ee5a10d-57f7-418f-82b3-2e97eba00647",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after epoch 0: 0.724625\n",
      "Cost after epoch 10: 0.724625\n",
      "Cost after epoch 20: 0.724625\n",
      "Cost after epoch 30: 0.724625\n",
      "Cost after epoch 40: 0.724625\n",
      "Cost after epoch 50: 0.724625\n",
      "Cost after epoch 60: 0.724625\n",
      "Cost after epoch 70: 0.724625\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m layers_dims = [\u001b[32m12288\u001b[39m, \u001b[32m20\u001b[39m, \u001b[32m7\u001b[39m, \u001b[32m5\u001b[39m, \u001b[32m1\u001b[39m] \n\u001b[32m      2\u001b[39m \u001b[38;5;66;03m#parameters = optimized_model(train_x, train_y, layers_dims, optimizer = \"adam\",num_epochs = 1000)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m parameters = \u001b[43moptimized_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlayers_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgd\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Predict\u001b[39;00m\n\u001b[32m      5\u001b[39m predictions = predict(train_x, train_y, parameters)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[11]\u001b[39m\u001b[32m, line 55\u001b[39m, in \u001b[36moptimized_model\u001b[39m\u001b[34m(X, Y, layers_dims, optimizer, learning_rate, mini_batch_size, beta, beta1, beta2, epsilon, num_epochs, print_cost)\u001b[39m\n\u001b[32m     52\u001b[39m (minibatch_X, minibatch_Y) = minibatch\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Forward propagation\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m AL, caches = \u001b[43mL_model_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mminibatch_X\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# Compute cost and add to the cost total\u001b[39;00m\n\u001b[32m     58\u001b[39m cost_total += compute_cost(AL, minibatch_Y)* minibatch_Y.shape[\u001b[32m1\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/My_projects /Damage_Vision_project/notebooks/Optimized_Model.py:346\u001b[39m, in \u001b[36mL_model_forward\u001b[39m\u001b[34m(X, parameters)\u001b[39m\n\u001b[32m    344\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[32m1\u001b[39m, L):\n\u001b[32m    345\u001b[39m     A_prev = A \n\u001b[32m--> \u001b[39m\u001b[32m346\u001b[39m     A, cache = \u001b[43mlinear_activation_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mW\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mb\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43ml\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mrelu\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    347\u001b[39m     caches.append(cache)\n\u001b[32m    349\u001b[39m \u001b[38;5;66;03m# Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\" list.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/My_projects /Damage_Vision_project/notebooks/Optimized_Model.py:313\u001b[39m, in \u001b[36mlinear_activation_forward\u001b[39m\u001b[34m(A_prev, W, b, activation)\u001b[39m\n\u001b[32m    309\u001b[39m     A, activation_cache = sigmoid(Z)\n\u001b[32m    311\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m activation == \u001b[33m\"\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    312\u001b[39m     \u001b[38;5;66;03m# Inputs: \"A_prev, W, b\". Outputs: \"A, activation_cache\".\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m313\u001b[39m     Z, linear_cache = \u001b[43mlinear_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA_prev\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    314\u001b[39m     A, activation_cache = relu(Z)\n\u001b[32m    316\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Desktop/My_projects /Damage_Vision_project/notebooks/Optimized_Model.py:282\u001b[39m, in \u001b[36mlinear_forward\u001b[39m\u001b[34m(A, W, b)\u001b[39m\n\u001b[32m    268\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlinear_forward\u001b[39m(A, W, b):\n\u001b[32m    269\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    270\u001b[39m \u001b[33;03m    Implement the linear part of a layer's forward propagation.\u001b[39;00m\n\u001b[32m    271\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    279\u001b[39m \u001b[33;03m    cache -- a python dictionary containing \"A\", \"W\" and \"b\" ; stored for computing the backward pass efficiently\u001b[39;00m\n\u001b[32m    280\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m282\u001b[39m     Z = \u001b[43mW\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mA\u001b[49m\u001b[43m)\u001b[49m + b\n\u001b[32m    284\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m(Z.shape == (W.shape[\u001b[32m0\u001b[39m], A.shape[\u001b[32m1\u001b[39m]))\n\u001b[32m    285\u001b[39m     cache = (A, W, b)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1] \n",
    "#parameters = optimized_model(train_x, train_y, layers_dims, optimizer = \"adam\",num_epochs = 1000)\n",
    "parameters = optimized_model(train_x, train_y, layers_dims, optimizer = \"gd\")\n",
    "# Predict\n",
    "predictions = predict(train_x, train_y, parameters)\n",
    "\n",
    "# # Plot decision boundary\n",
    "# plt.title(\"Model with Adam optimization\")\n",
    "# axes = plt.gca()\n",
    "# axes.set_xlim([-1.5,2.5])\n",
    "# axes.set_ylim([-1,1.5])\n",
    "# plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8a4d6a8c-0bd5-44f3-9262-19f60c0f07b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.4831804281345565\n"
     ]
    }
   ],
   "source": [
    "predictions = predict(test_x, test_y, parameters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fa6075f-96d4-4ee5-95ec-3d9ba316bb82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6b3844f-830f-43b3-94bb-170667d966f5",
   "metadata": {},
   "source": [
    "## Optimized Model: Rethinking the Pipeline\n",
    "\n",
    "In the previous notebooks, I implemented multiple models of increasing complexity, ranging from logistic regression to deeper neural networks. While these experiments were valuable for understanding core concepts in machine learning and deep learning, the resulting performance remained relatively low and inconsistent.\n",
    "\n",
    "After analyzing the results more carefully, it became clear that the limitation was not only related to model architecture or optimization techniques, but also to **how the data was labeled and interpreted**.\n",
    "\n",
    "### Identified Issue with Data Labeling\n",
    "\n",
    "In the earlier approach, an image was classified as *damaged* if **any single polygon** within the image was labeled as `D_Building` or `Debris`. This means that even a small, localized damaged region could cause the entire image to be labeled as damaged.  \n",
    "Such a strategy likely introduces noise and label ambiguity, especially for images that are largely intact but contain minor damage.\n",
    "\n",
    "This coarse labeling scheme may prevent the model from learning meaningful visual patterns related to *overall structural damage*, which is the core objective of this project.\n",
    "\n",
    "### Objective of This Notebook\n",
    "\n",
    "In this notebook, I aim to build the **most optimized model so far**, not only by:\n",
    "- improving model architecture,\n",
    "- applying better initialization, regularization, and optimization techniques,\n",
    "\n",
    "but also by **revisiting and refining the data labeling strategy itself**.\n",
    "\n",
    "By aligning the labels more closely with the true semantic meaning of structural damage, the goal is to provide the model with cleaner supervision and enable more reliable learning.\n",
    "\n",
    "This step marks a transition from experimenting with models to **systematically improving the full machine learning pipeline**, from data understanding to final evaluation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344e76b5-52c2-4c6a-b0cb-e65bad887e86",
   "metadata": {},
   "source": [
    "## Parsing the XML file & Preparing the data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1bae27b1-943d-4ae1-9265-cfbed5fd3061",
   "metadata": {},
   "outputs": [],
   "source": [
    "from Optimized_Model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c867baa-be67-47fc-b762-59fcf6120a30",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_train = parse_destroyed_with_size_check(\"../EIDSeg_Dataset/data/train/train.xml\", min_coverage=0.3)\n",
    "labels_test  = parse_destroyed_with_size_check(\"../EIDSeg_Dataset/data/test/test.xml\",  min_coverage=0.3)\n",
    "\n",
    "X_train_org, ordered_filenames_train = load_and_resize_images(\"../EIDSeg_Dataset/data/train/images/default\", target_size=(64,64))\n",
    "X_test_org,  ordered_filenames_test  = load_and_resize_images(\"../EIDSeg_Dataset/data/test/images/default\",  target_size=(64,64))\n",
    "\n",
    "Y_train_org = build_label_array(ordered_filenames_train, labels_train)   # shape (n_train,)\n",
    "Y_test_org  = build_label_array(ordered_filenames_test,  labels_test)    # shape (n_test,)\n",
    "\n",
    "# quick sanity checks\n",
    "print(\"Train positive ratio:\", Y_train_org.mean(), \"n_train:\", Y_train_org.shape[0])\n",
    "print(\"Test  positive ratio:\", Y_test_org.mean(),  \"n_test:\",  Y_test_org.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d128233-0e35-43ce-bc87-9c19bc96fc91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(12288, 2612) (1, 2612)\n",
      "(12288, 327) (1, 327)\n"
     ]
    }
   ],
   "source": [
    "train_x = X_train_org.reshape(X_train_org.shape[0], -1).T\n",
    "train_y = Y_train_org.reshape(1,-1)\n",
    "test_x = X_test_org.reshape(X_test_org.shape[0], -1).T\n",
    "test_y = Y_test_org.reshape(1,-1)\n",
    "\n",
    "\n",
    "print(train_x.shape, train_y.shape)\n",
    "print(test_x.shape, test_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "059cb6e9-f824-43b2-8ed3-b4483d7e746c",
   "metadata": {},
   "source": [
    "## L-layer Neural Network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "6e746df3-1539-452d-b594-1c834d839d0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "        \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "                \n",
    "        # Compute cost.\n",
    "        cost = compute_cost(AL,Y)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward(AL, Y, caches)\n",
    "        \n",
    "        # Update parameters.\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                        \n",
    "        # Print the cost every 100 iterations and for the last iteration\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a809389d-254c-4202-a050-ed94ccb1512d",
   "metadata": {},
   "source": [
    "## Training!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0439af21-1294-445d-8e22-50a53f36c9ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.7246250162012067\n",
      "Cost after iteration 100: 0.6872203231761475\n",
      "Cost after iteration 200: 0.6775461381963849\n",
      "Cost after iteration 300: 0.6660236810493122\n",
      "Cost after iteration 400: 0.675322633125315\n",
      "Cost after iteration 500: 0.6539324891845053\n",
      "Cost after iteration 600: 0.6572655245114436\n",
      "Cost after iteration 700: 0.6285363185367053\n",
      "Cost after iteration 800: 0.6369991025692343\n",
      "Cost after iteration 900: 0.6473037728599877\n",
      "Cost after iteration 1000: 0.5995318317603917\n",
      "Cost after iteration 1100: 0.61476440088501\n",
      "Cost after iteration 1200: 0.6015165843621063\n",
      "Cost after iteration 1300: 0.5705049178075906\n",
      "Cost after iteration 1400: 0.5649023718327169\n",
      "Cost after iteration 1500: 0.5608144433536896\n",
      "Cost after iteration 1600: 0.5818106456401206\n",
      "Cost after iteration 1700: 0.6282697297242217\n",
      "Cost after iteration 1800: 0.5505934307406615\n",
      "Cost after iteration 1900: 0.5623603370943553\n",
      "Cost after iteration 2000: 0.5638067592656174\n",
      "Cost after iteration 2100: 0.5239095162844934\n",
      "Cost after iteration 2200: 0.699562275389274\n",
      "Cost after iteration 2300: 0.4850974434355157\n",
      "Cost after iteration 2400: 0.5930385315473986\n",
      "Cost after iteration 2499: 0.573857897708467\n"
     ]
    }
   ],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1] \n",
    "parameters, costs = L_layer_model(train_x, train_y, layers_dims,learning_rate = 0.009, num_iterations = 2500, print_cost = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "05d8a8fb-fdd5-4aac-b7e5-4258d1e6a4cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train :Accuracy: 0.8016845329249619\n",
      "Test :Accuracy: 0.5382262996941896\n"
     ]
    }
   ],
   "source": [
    "print(\"Train\", end= \" \")\n",
    "pred_train = predict(train_x, train_y, parameters)\n",
    "print(\"Test\",end= \" \")\n",
    "pred_test = predict(test_x, test_y, parameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ad6d16c-9429-41ea-890b-fddaf96a9436",
   "metadata": {},
   "source": [
    "## Let's try Mini-Batch with Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f0abe-111f-41ec-9975-1ca8ba52ec03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X, Y, layers_dims, optimizer, learning_rate = 0.0007, mini_batch_size = 64, beta = 0.9,\n",
    "          beta1 = 0.9, beta2 = 0.999,  epsilon = 1e-8, num_epochs = 5000, print_cost = True):\n",
    "    \"\"\"\n",
    "    3-layer neural network model which can be run in different optimizer modes.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (2, number of examples)\n",
    "    Y -- true \"label\" vector (1 for blue dot / 0 for red dot), of shape (1, number of examples)\n",
    "    optimizer -- the optimizer to be passed, gradient descent, momentum or adam\n",
    "    layers_dims -- python list, containing the size of each layer\n",
    "    learning_rate -- the learning rate, scalar.\n",
    "    mini_batch_size -- the size of a mini batch\n",
    "    beta -- Momentum hyperparameter\n",
    "    beta1 -- Exponential decay hyperparameter for the past gradients estimates \n",
    "    beta2 -- Exponential decay hyperparameter for the past squared gradients estimates \n",
    "    epsilon -- hyperparameter preventing division by zero in Adam updates\n",
    "    num_epochs -- number of epochs\n",
    "    print_cost -- True to print the cost every 1000 epochs\n",
    "\n",
    "    Returns:\n",
    "    parameters -- python dictionary containing your updated parameters \n",
    "    \"\"\"\n",
    "\n",
    "    L = len(layers_dims)             # number of layers in the neural networks\n",
    "    costs = []                       # to keep track of the cost\n",
    "    t = 0                            # initializing the counter required for Adam update\n",
    "    seed = 10                        # For grading purposes, so that your \"random\" minibatches are the same as ours\n",
    "    m = X.shape[1]                   # number of training examples\n",
    "    \n",
    "    # Initialize parameters\n",
    "    parameters = initialize_parameters(layers_dims)\n",
    "\n",
    "    # Initialize the optimizer\n",
    "    if optimizer == \"gd\":\n",
    "        pass # no initialization required for gradient descent\n",
    "    elif optimizer == \"momentum\":\n",
    "        v = initialize_velocity(parameters)\n",
    "    elif optimizer == \"adam\":\n",
    "        v, s = initialize_adam(parameters)\n",
    "    \n",
    "    # Optimization loop\n",
    "    for i in range(num_epochs):\n",
    "        \n",
    "        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n",
    "        seed = seed + 1\n",
    "        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n",
    "        cost_total = 0\n",
    "        \n",
    "        for minibatch in minibatches:\n",
    "\n",
    "            # Select a minibatch\n",
    "            (minibatch_X, minibatch_Y) = minibatch\n",
    "\n",
    "            # Forward propagation\n",
    "            a3, caches = forward_propagation(minibatch_X, parameters)\n",
    "\n",
    "            # Compute cost and add to the cost total\n",
    "            cost_total += compute_cost(a3, minibatch_Y)\n",
    "\n",
    "            # Backward propagation\n",
    "            grads = backward_propagation(minibatch_X, minibatch_Y, caches)\n",
    "\n",
    "            # Update parameters\n",
    "            if optimizer == \"gd\":\n",
    "                parameters = update_parameters_with_gd(parameters, grads, learning_rate)\n",
    "            elif optimizer == \"momentum\":\n",
    "                parameters, v = update_parameters_with_momentum(parameters, grads, v, beta, learning_rate)\n",
    "            elif optimizer == \"adam\":\n",
    "                t = t + 1 # Adam counter\n",
    "                parameters, v, s, _, _ = update_parameters_with_adam(parameters, grads, v, s,\n",
    "                                                               t, learning_rate, beta1, beta2,  epsilon)\n",
    "        cost_avg = cost_total / m\n",
    "        \n",
    "        # Print the cost every 1000 epoch\n",
    "        if print_cost and i % 1000 == 0:\n",
    "            print (\"Cost after epoch %i: %f\" %(i, cost_avg))\n",
    "        if print_cost and i % 100 == 0:\n",
    "            costs.append(cost_avg)\n",
    "                \n",
    "    # plot the cost\n",
    "    plt.plot(costs)\n",
    "    plt.ylabel('cost')\n",
    "    plt.xlabel('epochs (per 100)')\n",
    "    plt.title(\"Learning rate = \" + str(learning_rate))\n",
    "    plt.show()\n",
    "\n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d466057-57d7-46be-8e24-bcfc00af8eee",
   "metadata": {},
   "source": [
    "### lets train !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ee5a10d-57f7-418f-82b3-2e97eba00647",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1] \n",
    "parameters = model(train_x, train_y, layers_dims, optimizer = \"adam\")\n",
    "# Predict\n",
    "predictions = predict(train_x, train_y, parameters)\n",
    "\n",
    "# Plot decision boundary\n",
    "plt.title(\"Model with Adam optimization\")\n",
    "axes = plt.gca()\n",
    "axes.set_xlim([-1.5,2.5])\n",
    "axes.set_ylim([-1,1.5])\n",
    "plot_decision_boundary(lambda x: predict_dec(parameters, x.T), train_X, train_Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a4d6a8c-0bd5-44f3-9262-19f60c0f07b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = predict(test_x, test_y, parameters)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

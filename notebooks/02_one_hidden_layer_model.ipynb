{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "944cea92-831b-43aa-bce4-8e3d00a78722",
   "metadata": {},
   "source": [
    "## The one-hidden-layer neural network outperformed logistic regression (79.33% train accuracy, 73.39% test accuracy), but accuracy remains below the required level. \n",
    "## Training was also slow because the code didn't run on the GPU. Future models will address these issues."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c591f187-fb63-4bbc-bf1e-bc9ba46b4286",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import numpy as np\n",
    "import h5py \n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from NN_1_hidden_layer_02.NN_1_hidden_layer_model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2cfbeb07-956e-45f6-948e-172328e8a173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: m_train = 2612\n",
      "Number of testing examples: m_test = 327\n",
      "Height/Width of each image: num_px = 64\n",
      "Each image is of size: (64, 64, 3)\n",
      "train_set_x shape: (2612, 64, 64, 3)\n",
      "train_set_y shape: (1, 2612)\n",
      "test_set_x shape: (327, 64, 64, 3)\n",
      "test_set_y shape: (1, 327)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"../EIDSeg_Dataset/cache/eidseg_64x64_binary_any.h5\", \"r\") as f:\n",
    "    X_train_org = f[\"X_train\"][:]\n",
    "    Y_train_org = f[\"Y_train\"][:]\n",
    "    X_test_org  = f[\"X_test\"][:]\n",
    "    Y_test_org  = f[\"Y_test\"][:]\n",
    "\n",
    "m_train = X_train_org.shape[0]\n",
    "m_test = X_test_org.shape[0]\n",
    "num_px =X_train_org.shape[1]\n",
    "\n",
    "print (\"Number of training examples: m_train = \" + str(m_train))\n",
    "print (\"Number of testing examples: m_test = \" + str(m_test))\n",
    "print (\"Height/Width of each image: num_px = \" + str(num_px))\n",
    "print (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\n",
    "print (\"train_set_x shape: \" + str(X_train_org.shape))\n",
    "print (\"train_set_y shape: \" + str(Y_train_org.shape))\n",
    "print (\"test_set_x shape: \" + str(X_test_org.shape))\n",
    "print (\"test_set_y shape: \" + str(Y_test_org.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a1f5cacd-65ef-4ac4-80b9-0f8246149ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_set_x flatten shape: (12288, 2612)\n",
      "train_set_y shape: (1, 2612)\n",
      "test_set_x flatten shape: (12288, 327)\n",
      "test_set_y shape: (1, 327)\n"
     ]
    }
   ],
   "source": [
    "with h5py.File(\"../EIDSeg_Dataset/cache//eidseg_64x64_binary_any_flat.h5\", \"r\") as f:\n",
    "    train_x = f[\"train_x\"][:]   # (12288, m)\n",
    "    train_y = f[\"train_y\"][:]   # (1, m)\n",
    "    test_x  = f[\"test_x\"][:]    # (12288, m)\n",
    "    test_y  = f[\"test_y\"][:]    # (1, m)\n",
    "\n",
    "print (\"train_set_x flatten shape: \" + str(train_x.shape))\n",
    "print (\"train_set_y shape: \" + str(train_y.shape))\n",
    "print (\"test_set_x flatten shape: \" + str(test_x.shape))\n",
    "print (\"test_set_y shape: \" + str(test_y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07cca48-e33b-4763-9211-bd22a97bde82",
   "metadata": {},
   "source": [
    "## nn_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "888353db-9450-4bd7-ac28-a5e01ff88abf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def nn_model(X_train, Y_train, n_h=128, num_iterations = 10000, learning_rate=0.001, print_cost=False, X_test=None, Y_test=None):\n",
    "    \"\"\"\n",
    "    Arguments:\n",
    "    X -- dataset of shape (2, number of examples)\n",
    "    Y -- labels of shape (1, number of examples)\n",
    "    n_h -- size of the hidden layer\n",
    "    num_iterations -- Number of iterations in gradient descent loop\n",
    "    print_cost -- if True, print the cost every 1000 iterations\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "    \n",
    "    #np.random.seed(3)\n",
    "    n_x = layer_sizes(X_train, Y_train)[0]\n",
    "    n_y = layer_sizes(X_train, Y_train)[2]\n",
    "    \n",
    "    (n_x, n_h, n_y) = layer_sizes(X_train, Y_train)\n",
    "    parameters = initialize_parameters(n_x, n_h, n_y)\n",
    "    costs = []\n",
    "\n",
    "\n",
    "    for i in range(0, num_iterations):\n",
    "        A2, cache = forward_propagation(X_train, parameters)\n",
    "\n",
    "        cost = compute_cost(A2, Y_train)\n",
    "        \n",
    "        grads = backward_propagation(parameters, cache, X_train, Y_train)\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "\n",
    "        # Print the cost every 500 iterations\n",
    "        if print_cost and (i % 500 == 0):\n",
    "            msg = f\"Cost after iteration {i}: {cost:.6f}\"\n",
    "            if X_test is not None and Y_test is not None:\n",
    "                train_pred = predict(parameters, X_train)\n",
    "                train_acc = accuracy(train_pred, Y_train)\n",
    "                msg += f\" | train acc: {train_acc:.2f}%\"\n",
    "            print(msg)\n",
    "    if X_test is not None and Y_test is not None:\n",
    "        train_pred = predict(parameters, X_train)\n",
    "        test_pred = predict(parameters, X_test)\n",
    "        train_acc = accuracy(train_pred, Y_train)\n",
    "        test_acc = accuracy(test_pred, Y_test)\n",
    "        print(f\"Final train accuracy: {train_acc:.2f}%\")\n",
    "        print(f\"Final test accuracy:  {test_acc:.2f}%\")\n",
    "\n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e32e8422-aa62-4f77-a710-e37833d35bb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0: 0.693071 | train acc: 70.71%\n",
      "Cost after iteration 500: 0.617860 | train acc: 70.71%\n",
      "Cost after iteration 1000: 0.606876 | train acc: 70.71%\n",
      "Cost after iteration 1500: 0.605093 | train acc: 70.71%\n",
      "Cost after iteration 2000: 0.604788 | train acc: 70.71%\n",
      "Cost after iteration 2500: 0.604734 | train acc: 70.71%\n",
      "Cost after iteration 3000: 0.604723 | train acc: 70.71%\n"
     ]
    }
   ],
   "source": [
    "params, costs = nn_model(train_x, Y_train_org,\n",
    "                  n_h=128,\n",
    "                  num_iterations=3500,\n",
    "                  learning_rate=0.008,\n",
    "                  print_cost=True,\n",
    "                  X_test=test_x,\n",
    "                  Y_test=Y_test_org)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef19e6ec-8692-4a92-bea5-dca888d8912a",
   "metadata": {},
   "source": [
    "# Clearly it can't learn more than that "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90dc2336-866a-43c8-96a3-35e7558a0baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from eval_utils import *\n",
    "from visualization import *\n",
    "\n",
    "print_report(params)\n",
    "plot_costs(costs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ddf290-1054-484b-bed9-c2952870749b",
   "metadata": {},
   "source": [
    "## as the last model has 0.934 train accuracy and 0.734 test accuracy it is clearly overfitting so I'll try regulazation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62c9ab78-5909-4376-8894-3805016a673e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnn_structure import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ee70d7-7e53-4d3d-b956-8b05b8eaff93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dnn_structure import *\n",
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from Load_data.xml_utils import parse_cvat_xml_all_labels, label_Y_binary\n",
    "from Load_data.data_loader import load_and_resize_images, build_label_array\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ef93b4-c5e8-407e-a027-1da54a071192",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "TRAIN_XML = \"../EIDSeg_Dataset/data/train/train.xml\"\n",
    "TEST_XML = \"../EIDSeg_Dataset/data/test/test.xml\"\n",
    "TRAIN_IMAGES = \"../EIDSeg_Dataset/data/train/images/default\"\n",
    "TEST_IMAGES = \"../EIDSeg_Dataset/data/test/images/default\"\n",
    "\n",
    "IMAGE_SIZE = (64, 64)\n",
    "NUM_ITER = 2000\n",
    "\n",
    "\n",
    "\n",
    "labels_train_raw = parse_cvat_xml_all_labels(TRAIN_XML)\n",
    "labels_test_raw = parse_cvat_xml_all_labels(TEST_XML)\n",
    "\n",
    "Y_train_map = label_Y_binary(labels_train_raw)\n",
    "Y_test_map = label_Y_binary(labels_test_raw)\n",
    "\n",
    "X_train_org, ordered_train = load_and_resize_images(TRAIN_IMAGES, size=IMAGE_SIZE)\n",
    "X_test_org, ordered_test = load_and_resize_images(TEST_IMAGES, size=IMAGE_SIZE)\n",
    "\n",
    "Y_train_org = build_label_array(ordered_train, Y_train_map)\n",
    "Y_test_org = build_label_array(ordered_test, Y_test_map)\n",
    "\n",
    "train_x = X_train_org.reshape(X_train_org.shape[0], -1).T\n",
    "test_x = X_test_org.reshape(X_test_org.shape[0], -1).T\n",
    "\n",
    "print(train_x.shape, Y_train_org.shape)\n",
    "print(test_x.shape, Y_test_org.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e03101-69b6-4313-af8c-4a211fcf0f73",
   "metadata": {},
   "source": [
    "## L2 Regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a00cdc-e630-4a6b-ae9c-4631fafe8603",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost_with_regularization(AL, Y, parameters, lambd):\n",
    "    \"\"\"\n",
    "    Implement the cost function with L2 regularization.\n",
    "\n",
    "    Arguments:\n",
    "    AL -- probability vector from forward propagation, shape (1, m)\n",
    "    Y -- true labels vector, shape (1, m)\n",
    "    parameters -- python dictionary containing W1...WL\n",
    "    lambd -- regularization parameter\n",
    "\n",
    "    Returns:\n",
    "    cost -- regularized cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    \n",
    "    # Cross-entropy cost\n",
    "    cross_entropy_cost = compute_cost(AL, Y)\n",
    "        \n",
    "    # L2 regularization cost\n",
    "    L2_regularization_cost = 0\n",
    "    L = len(parameters) // 2  # number of layers\n",
    "    \n",
    "    for l in range(1, L + 1):\n",
    "        L2_regularization_cost += np.sum(np.square(parameters[\"W\" + str(l)]))\n",
    "    \n",
    "    L2_regularization_cost *= (lambd / (2 * m))\n",
    "    \n",
    "    cost = cross_entropy_cost + L2_regularization_cost\n",
    "    \n",
    "    return cost\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3663999-74aa-487f-90fe-ec2ad4b8e11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_backward_with_regularization(AL, Y, caches, lambd):\n",
    "    \"\"\"\n",
    "    Implement backward propagation with L2 regularization for [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of forward propagation (1, m)\n",
    "    Y -- true labels vector (1, m)\n",
    "    caches -- list of caches from L_model_forward\n",
    "    lambd -- regularization parameter\n",
    "    \n",
    "    Returns:\n",
    "    grads -- dictionary with gradients\n",
    "    \"\"\"\n",
    "    \n",
    "    grads = {}\n",
    "    L = len(caches)           # number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape)\n",
    "    \n",
    "    # ---------- OUTPUT LAYER (SIGMOID) ----------\n",
    "    current_cache = caches[L - 1]\n",
    "    \n",
    "    dAL = - (np.divide(Y, AL) - np.divide(1 - Y, 1 - AL))\n",
    "    \n",
    "    dA_prev, dW, db = linear_activation_backward(\n",
    "        dAL, current_cache, activation=\"sigmoid\"\n",
    "    )\n",
    "    \n",
    "    grads[\"dW\" + str(L)] = dW + (lambd / m) * current_cache[0][1]\n",
    "    grads[\"db\" + str(L)] = db\n",
    "    grads[\"dA\" + str(L - 1)] = dA_prev\n",
    "    \n",
    "    # ---------- HIDDEN LAYERS (RELU) ----------\n",
    "    for l in reversed(range(L - 1)):\n",
    "        current_cache = caches[l]\n",
    "        \n",
    "        dA_prev, dW, db = linear_activation_backward(\n",
    "            grads[\"dA\" + str(l + 1)], current_cache, activation=\"relu\"\n",
    "        )\n",
    "        \n",
    "        grads[\"dW\" + str(l + 1)] = dW + (lambd / m) * current_cache[0][1]\n",
    "        grads[\"db\" + str(l + 1)] = db\n",
    "        grads[\"dA\" + str(l)] = dA_prev\n",
    "    \n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54a5b893-9de9-4a56-a5f9-2d24e4c082da",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def L_layer_model(X, Y, layers_dims, learning_rate = 0.0075, num_iterations = 3000, print_cost=False):\n",
    "    \"\"\"\n",
    "    Implements a L-layer neural network: [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID.\n",
    "    \n",
    "    Arguments:\n",
    "    X -- input data, of shape (n_x, number of examples)\n",
    "    Y -- true \"label\" vector (containing 1 if cat, 0 if non-cat), of shape (1, number of examples)\n",
    "    layers_dims -- list containing the input size and each layer size, of length (number of layers + 1).\n",
    "    learning_rate -- learning rate of the gradient descent update rule\n",
    "    num_iterations -- number of iterations of the optimization loop\n",
    "    print_cost -- if True, it prints the cost every 100 steps\n",
    "    \n",
    "    Returns:\n",
    "    parameters -- parameters learnt by the model. They can then be used to predict.\n",
    "    \"\"\"\n",
    "\n",
    "    np.random.seed(1)\n",
    "    costs = []                         # keep track of cost\n",
    "    \n",
    "    # Parameters initialization.\n",
    "\n",
    "    parameters = initialize_parameters_deep(layers_dims)\n",
    "        \n",
    "    # Loop (gradient descent)\n",
    "    for i in range(0, num_iterations):\n",
    "        # Forward propagation: [LINEAR -> RELU]*(L-1) -> LINEAR -> SIGMOID.\n",
    "        AL, caches = L_model_forward(X, parameters)\n",
    "                \n",
    "        # Compute cost.\n",
    "        cost = compute_cost_with_regularization(AL, Y, parameters, lambd=0.7)\n",
    "            \n",
    "        # Backward propagation.\n",
    "        grads = L_model_backward_with_regularization(AL, Y, caches, lambd=0.7)\n",
    "        \n",
    "        # Update parameters.\n",
    "\n",
    "        parameters = update_parameters(parameters, grads, learning_rate)\n",
    "                        \n",
    "        # Print the cost every 100 iterations and for the last iteration\n",
    "        if print_cost and (i % 100 == 0 or i == num_iterations - 1):\n",
    "            print(\"Cost after iteration {}: {}\".format(i, np.squeeze(cost)))\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "    \n",
    "    return parameters, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72e60ced-bd69-4c36-9349-be6fed00782d",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_dims = [12288, 20, 7, 5, 1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "708275a5-b436-4c95-a298-5c7cb1fc9728",
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters, costs = L_layer_model(train_x, Y_train_org, layers_dims,learning_rate = 0.008, num_iterations = 3500, print_cost = True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
